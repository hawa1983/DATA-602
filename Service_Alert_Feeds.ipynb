{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMji0vttIL2d7+UaiXs+Sic",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hawa1983/DATA-602/blob/main/Service_Alert_Feeds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gtfs-realtime-bindings\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "HqR9USqeoqaW",
        "outputId": "f1394493-c5eb-47ac-b721-f2c99960269e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gtfs-realtime-bindings\n",
            "  Downloading gtfs-realtime-bindings-1.0.0.tar.gz (6.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from gtfs-realtime-bindings) (75.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from gtfs-realtime-bindings) (5.29.5)\n",
            "Building wheels for collected packages: gtfs-realtime-bindings\n",
            "  Building wheel for gtfs-realtime-bindings (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gtfs-realtime-bindings: filename=gtfs_realtime_bindings-1.0.0-py3-none-any.whl size=5987 sha256=8fd51ced4c81af7ef875058552b5a2fa5acf8210d85c47344fad783e96a1ece1\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/43/38/17a10a2cdd30cb86acceb42e24e7d2d6bb98b2c59ff8983e20\n",
            "Successfully built gtfs-realtime-bindings\n",
            "Installing collected packages: gtfs-realtime-bindings\n",
            "Successfully installed gtfs-realtime-bindings-1.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "55a454dd1f1e48a78b8ee080ce570add"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Code Overview: MTA GTFS-Realtime Subway Alerts Parser**\n",
        "\n",
        "This script connects to the **Metropolitan Transportation Authority (MTA)**‚Äôs **GTFS-Realtime ‚Äúsubway-alerts‚Äù feed**, retrieves live service alerts in **Protocol Buffers (protobuf)** format, and converts them into a structured, analysis-ready **pandas DataFrame**.\n",
        "\n",
        "GTFS-Realtime feeds provide continuously updated information on **service changes, delays, and planned work** across subway lines. The feed encodes alerts in a compact binary format defined by Google‚Äôs GTFS-Realtime specification (`gtfs_realtime.proto`). This script handles the complete workflow:\n",
        "\n",
        "1. **Fetches the raw protobuf feed** from the MTA API using multiple candidate URLs (to handle encoding inconsistencies such as `%2F`).\n",
        "2. **Parses the binary data** into human-readable fields such as route ID, stop ID, trip ID, active time period, cause, effect, and severity.\n",
        "3. **Flags active alerts** based on current timestamps, allowing integration with delay or headway datasets.\n",
        "4. **Outputs a structured `pandas` DataFrame**, which can be merged with historical performance data to study relationships between alerts and operational delays.\n",
        "\n",
        "This code forms the **data ingestion and transformation foundation** for modeling and visualization tasks in the MTA capstone project ‚Äî such as predicting delay classes, identifying systemic service issues, or evaluating the operational impact of different alert types.\n",
        "\n",
        "Excellent ‚Äî here‚Äôs a well-formatted **Data Dictionary** section that goes right after your introductory text and before the code block.\n",
        "It‚Äôs written in clear academic style, ready for inclusion in your Capstone report or notebook.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä **Data Dictionary: Parsed GTFS-Realtime Subway Alerts**\n",
        "\n",
        "| **Column Name**  | **Data Type**    | **Description / Meaning**                                                                                                                        | **Example Value**                                                 |\n",
        "| ---------------- | ---------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------- |\n",
        "| `alert_id`       | *string*         | Unique identifier assigned by MTA for each alert record. Used to track or deduplicate alerts across feeds.                                       | `\"1234567890\"`                                                    |\n",
        "| `is_active_now`  | *boolean*        | Indicates whether the alert is currently active at the time of data retrieval (`True` if now ‚àà [start_ts, end_ts]).                              | `True`                                                            |\n",
        "| `start_ts`       | *datetime (UTC)* | Start time when the alert became effective, converted from UNIX timestamp.                                                                       | `2025-10-15 10:00:00+00:00`                                       |\n",
        "| `end_ts`         | *datetime (UTC)* | End time when the alert expires or is scheduled to end.                                                                                          | `2025-10-15 14:00:00+00:00`                                       |\n",
        "| `route_id`       | *string*         | Identifier for the affected subway line(s). Corresponds to route IDs in the GTFS static feed (e.g., ‚ÄúA‚Äù, ‚Äú6‚Äù, ‚ÄúQ‚Äù).                              | `\"D\"`                                                             |\n",
        "| `stop_id`        | *string / None*  | ID of the affected stop or station, if specified in the alert‚Äôs informed entity.                                                                 | `\"R14\"`                                                           |\n",
        "| `trip_id`        | *string / None*  | ID of the affected train trip, if provided. Useful for mapping specific runs or schedules.                                                       | `\"067800_A..S05R\"`                                                |\n",
        "| `header`         | *string*         | Short human-readable summary of the alert, intended for passenger display.                                                                       | `\"Delays on the D line\"`                                          |\n",
        "| `description`    | *string*         | Longer, detailed description of the service issue or advisory.                                                                                   | `\"Signal problems at 125 St ‚Äî expect delays in both directions.\"` |\n",
        "| `cause`          | *categorical*    | Programmatic cause of the alert (enumeration from GTFS spec). Examples include `SIGNAL_PROBLEM`, `TRACK_MAINTENANCE`, `WEATHER`, etc.            | `\"SIGNAL_PROBLEM\"`                                                |\n",
        "| `effect`         | *categorical*    | Operational effect of the alert (enumeration from GTFS spec). Typical values include `NO_SERVICE`, `REDUCED_SERVICE`, `SIGNIFICANT_DELAYS`, etc. | `\"SIGNIFICANT_DELAYS\"`                                            |\n",
        "| `severity_level` | *categorical*    | Alert severity indicator, mapped to one of the GTFS-Realtime `SeverityLevel` enum values: `INFO`, `WARNING`, or `SEVERE`.                        | `\"SEVERE\"`                                                        |\n",
        "\n",
        "**Usage Note:**\n",
        "This structured DataFrame can be:\n",
        "\n",
        "* **Merged** with train movement or headway datasets using `route_id` and temporal windows.\n",
        "* **Aggregated** to compute alert frequency by route or severity level.\n",
        "* **Labeled** to support supervised learning models predicting `delay_class` or `EWT` outcomes.\n"
      ],
      "metadata": {
        "id": "HB0Fw9hHwbI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# MTA Subway Alerts Feed Parser\n",
        "# Author: Fomba (Sahr) Kassoh\n",
        "# Purpose: Fetch and parse GTFS-Realtime service alerts from the MTA API\n",
        "# Output: Structured pandas DataFrame of active and past alerts\n",
        "# ============================================================\n",
        "\n",
        "# --- Import necessary libraries ---\n",
        "import requests                      # For making HTTP requests to MTA API\n",
        "import pandas as pd                  # For storing and manipulating tabular data\n",
        "from datetime import datetime, timezone  # For handling UTC timestamps\n",
        "from google.transit import gtfs_realtime_pb2  # For parsing GTFS-Realtime protobuf data\n",
        "\n",
        "# ============================================================\n",
        "# Define possible GTFS-Realtime alert feed URLs.\n",
        "# MTA sometimes encodes the \"/\" character differently (%2F), so\n",
        "# we include multiple variants to ensure at least one works.\n",
        "# ============================================================\n",
        "CANDIDATE_URLS = [\n",
        "    \"https://api-endpoint.mta.info/Dataservice/mtagtfsfeeds/camsys/subway-alerts\",\n",
        "    \"https://api-endpoint.mta.info/Dataservice/mtagtfsfeeds/camsys%2Fsubway-alerts\",\n",
        "    \"https://api-endpoint.mta.info/Dataservice/mtagtfsfeeds/camsys/all-alerts\",\n",
        "    \"https://api-endpoint.mta.info/Dataservice/mtagtfsfeeds/camsys%2Fall-alerts\",\n",
        "]\n",
        "\n",
        "# ============================================================\n",
        "# Define HTTP headers for the request.\n",
        "# - The \"User-Agent\" header tricks the server into thinking the\n",
        "#   request is coming from a web browser (avoids 403 errors).\n",
        "# - The \"Accept\" header tells the server we expect protobuf data.\n",
        "# ============================================================\n",
        "HEADERS = {\n",
        "    \"User-Agent\": (\n",
        "        \"Mozilla/5.0 (X11; Linux x86_64) \"\n",
        "        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0 Safari/537.36\"\n",
        "    ),\n",
        "    \"Accept\": \"application/x-protobuf\",\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# Function: fetch_feed()\n",
        "# Purpose:  Attempt to download the GTFS-Realtime feed from the\n",
        "#           list of candidate URLs, handling errors gracefully.\n",
        "# Returns:  Binary protobuf content (feed bytes)\n",
        "# ============================================================\n",
        "def fetch_feed():\n",
        "    last_err = None\n",
        "    for url in CANDIDATE_URLS:\n",
        "        try:\n",
        "            # Send GET request with custom headers\n",
        "            r = requests.get(url, headers=HEADERS, timeout=30)\n",
        "\n",
        "            # Raise HTTPError if the response code is not 200 (OK)\n",
        "            r.raise_for_status()\n",
        "\n",
        "            # Return the raw content (protobuf binary)\n",
        "            return r.content\n",
        "        except requests.HTTPError as e:\n",
        "            # Save the last error and try the next URL\n",
        "            last_err = e\n",
        "            continue\n",
        "\n",
        "    # If all attempts failed, raise the last error encountered\n",
        "    raise last_err\n",
        "\n",
        "# ============================================================\n",
        "# Function: parse_alerts(pb_bytes)\n",
        "# Purpose:  Parse the binary protobuf feed into a structured\n",
        "#           pandas DataFrame with alert details.\n",
        "# Input:    pb_bytes - raw protobuf bytes returned by fetch_feed()\n",
        "# Output:   pandas DataFrame of alert records\n",
        "# ============================================================\n",
        "def parse_alerts(pb_bytes):\n",
        "    # Create a GTFS FeedMessage object and parse binary data\n",
        "    feed = gtfs_realtime_pb2.FeedMessage()\n",
        "    feed.ParseFromString(pb_bytes)\n",
        "\n",
        "    # --- Helper function: convert UNIX timestamp ‚Üí datetime ---\n",
        "    def _ts(x):\n",
        "        return None if not x else datetime.fromtimestamp(int(x), tz=timezone.utc)\n",
        "\n",
        "    # --- Helper function: extract translated text (string) ---\n",
        "    def _txt(ts):\n",
        "        return ts.translation[0].text if ts and ts.translation else None\n",
        "\n",
        "    # Prepare to store parsed alert rows\n",
        "    rows = []\n",
        "    now = datetime.now(tz=timezone.utc).timestamp()  # current UTC time in seconds\n",
        "\n",
        "    # ============================================================\n",
        "    # Loop through each entity in the GTFS feed.\n",
        "    # Each entity can be an alert, trip update, or vehicle position.\n",
        "    # We're only interested in alert entities.\n",
        "    # ============================================================\n",
        "    for ent in feed.entity:\n",
        "        if not ent.HasField(\"alert\"):\n",
        "            continue  # skip non-alert entities\n",
        "\n",
        "        a = ent.alert  # shorthand for the alert object\n",
        "\n",
        "        # Extract alert metadata\n",
        "        periods = a.active_period or [gtfs_realtime_pb2.TimeRange()]  # start/end times\n",
        "        informed = a.informed_entity or [gtfs_realtime_pb2.EntitySelector()]  # affected routes/trips/stops\n",
        "\n",
        "        # Map numeric enum values to human-readable names\n",
        "        cause  = gtfs_realtime_pb2.Alert.Cause.Name(a.cause) if a.HasField(\"cause\") else None\n",
        "        effect = gtfs_realtime_pb2.Alert.Effect.Name(a.effect) if a.HasField(\"effect\") else None\n",
        "        sev    = gtfs_realtime_pb2.Alert.SeverityLevel.Name(a.severity_level) if a.HasField(\"severity_level\") else None\n",
        "\n",
        "        # ============================================================\n",
        "        # Iterate through all active time periods and affected entities\n",
        "        # to generate one row per (alert √ó entity √ó time period)\n",
        "        # ============================================================\n",
        "        for ap in periods:\n",
        "            # Determine if the alert is currently active\n",
        "            active = (ap.start or 0) <= now <= (ap.end or 2**31 - 1)\n",
        "\n",
        "            for inf in informed:\n",
        "                # Build a row dictionary for each affected entity\n",
        "                rows.append({\n",
        "                    \"alert_id\": ent.id,  # unique alert identifier\n",
        "                    \"is_active_now\": bool(active),  # whether it's active now\n",
        "                    \"start_ts\": _ts(ap.start),      # start time (UTC datetime)\n",
        "                    \"end_ts\": _ts(ap.end),          # end time (UTC datetime)\n",
        "                    \"route_id\": getattr(inf, \"route_id\", None) or None,  # route affected (e.g., 'A', 'D', '6')\n",
        "                    \"stop_id\": getattr(inf, \"stop_id\", None) or None,    # specific stop (if applicable)\n",
        "                    # extract trip_id safely (nested object)\n",
        "                    \"trip_id\": getattr(inf, \"trip\", None).trip_id if hasattr(inf, \"trip\") and inf.trip.trip_id else None,\n",
        "                    \"header\": _txt(a.header_text),            # short alert summary\n",
        "                    \"description\": _txt(a.description_text),  # detailed description\n",
        "                    \"cause\": cause,                           # cause (e.g., SIGNAL_PROBLEM)\n",
        "                    \"effect\": effect,                         # effect (e.g., SIGNIFICANT_DELAYS)\n",
        "                    \"severity_level\": sev,                    # severity level (INFO, WARNING, SEVERE)\n",
        "                })\n",
        "\n",
        "    # Convert the collected list of dictionaries into a pandas DataFrame\n",
        "    # and drop duplicate rows that may occur due to multiple selectors.\n",
        "    return pd.DataFrame(rows).drop_duplicates()\n",
        "\n",
        "# ============================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Step 1: Fetch live GTFS-Realtime alerts feed from MTA API\n",
        "    pb = fetch_feed()\n",
        "\n",
        "    # Step 2: Parse the protobuf feed into a structured DataFrame\n",
        "    alerts_df = parse_alerts(pb)\n",
        "\n",
        "    # Step 3: Display the first few rows as a sanity check\n",
        "    print(alerts_df.head())\n",
        "\n",
        "# ============================================================\n",
        "# SAMPLE OUTPUT (structure)\n",
        "# ============================================================\n",
        "#   alert_id  is_active_now           start_ts              end_ts route_id  stop_id  trip_id  header                description              cause          effect             severity_level\n",
        "# 0 12345678  True            2025-10-15 10:00:00 2025-10-15 14:00:00   D       None     None     \"Delays on D Line\"  \"Signal problems at 125 St\"  SIGNAL_PROBLEM SIGNIFICANT_DELAYS SEVERE\n",
        "# ============================================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUdHY6oFv26_",
        "outputId": "2cf9df7f-6ef4-48ab-ecf8-ecf48bd78a48"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           alert_id  is_active_now                  start_ts end_ts route_id  \\\n",
            "0  lmm:alert:474838           True 2025-10-15 16:36:05+00:00    NaT        A   \n",
            "1  lmm:alert:474838           True 2025-10-15 16:36:05+00:00    NaT        C   \n",
            "2  lmm:alert:474838           True 2025-10-15 16:36:05+00:00    NaT        E   \n",
            "3  lmm:alert:474838           True 2025-10-15 16:36:05+00:00    NaT     None   \n",
            "4  lmm:alert:474835           True 2025-10-15 15:32:44+00:00    NaT        D   \n",
            "\n",
            "  stop_id trip_id                                             header  \\\n",
            "0    None    None  Uptown [A][C][E] trains are running with delay...   \n",
            "1    None    None  Uptown [A][C][E] trains are running with delay...   \n",
            "2    None    None  Uptown [A][C][E] trains are running with delay...   \n",
            "3     A27    None  Uptown [A][C][E] trains are running with delay...   \n",
            "4    None    None  Uptown [D] trains are running with delays whil...   \n",
            "\n",
            "                                         description cause effect  \\\n",
            "0  Uptown [A] trains have resumed making express ...  None   None   \n",
            "1  Uptown [A] trains have resumed making express ...  None   None   \n",
            "2  Uptown [A] trains have resumed making express ...  None   None   \n",
            "3  Uptown [A] trains have resumed making express ...  None   None   \n",
            "4                                               None  None   None   \n",
            "\n",
            "  severity_level  \n",
            "0           None  \n",
            "1           None  \n",
            "2           None  \n",
            "3           None  \n",
            "4           None  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What this code does (purpose ‚Üí method ‚Üí output)\n",
        "\n",
        "## Purpose\n",
        "\n",
        "Turn raw **alerts** (from your GTFS-RT parser) into **time-aligned features** you can join to your **headway/delay fact table**. The result is one row per `(timestamp_bin, route_id, stop_id)` with:\n",
        "\n",
        "* a binary `incident_flag`\n",
        "* one-hot flags for alert **severity** and **effect**\n",
        "* the most common `alert_type` and `alert_scope` in that bin\n",
        "\n",
        "These become model inputs for classification/regression (e.g., `delay_class`, `EWT`).\n",
        "\n",
        "---\n",
        "\n",
        "## Methodology (step-by-step)\n",
        "\n",
        "1. **Inputs (assumed schemas)**\n",
        "\n",
        "   * `alerts_df`: columns like `alert_id, start_ts, end_ts, route_id, stop_id, header, description, cause, effect, severity_level, is_active_now`\n",
        "   * `fact_df`: your target index with `timestamp_bin` (UTC), `route_id`, `stop_id`\n",
        "\n",
        "2. **Scope detection**\n",
        "\n",
        "   * `_alert_scope(row)`:\n",
        "\n",
        "     * if `stop_id` present ‚Üí `\"stop\"` (very specific)\n",
        "     * else if `route_id` present ‚Üí `\"route\"` (line-level)\n",
        "     * else ‚Üí `\"system\"` (agency-wide)\n",
        "\n",
        "3. **Normalization & tokens**\n",
        "\n",
        "   * Convert `start_ts`/`end_ts` to timezone-aware UTC datetimes.\n",
        "   * Uppercase categorical tokens:\n",
        "\n",
        "     * `severity_token` from `severity_level`\n",
        "     * `effect_token` from `effect`\n",
        "\n",
        "4. **Light NLP for `alert_type`**\n",
        "\n",
        "   * Build a coarse `alert_type` by scanning `header/description`:\n",
        "\n",
        "     * contains ‚Äúplanned work‚Äù/‚Äúmaintenance‚Äù ‚Üí `PLANNED_WORK`\n",
        "     * contains ‚Äúdelay‚Äù/‚Äúrunning local‚Äù ‚Üí `DELAYS`\n",
        "     * contains ‚Äúno service‚Äù/‚Äúsuspended‚Äù ‚Üí `NO_SERVICE`\n",
        "     * otherwise ‚Üí `GENERAL`\n",
        "   * (You can swap in a richer classifier later.)\n",
        "\n",
        "5. **Temporal expansion into bins**\n",
        "\n",
        "   * For each alert, generate a **date_range** of bin timestamps between `start_ts` and `end_ts` using `bin_freq` (default `\"10min\"`).\n",
        "   * If `end_ts` is missing (open-ended alert), cap it at **now + `horizon_hours`** (default 6h) so bins don‚Äôt run forever.\n",
        "\n",
        "6. **Emit rows by scope**\n",
        "\n",
        "   * For **route/system** scope: emit `(timestamp_bin, route_id, stop_id=None, ‚Ä¶)`\n",
        "   * For **stop** scope: emit `(timestamp_bin, route_id, stop_id, ‚Ä¶)`\n",
        "   * Keep `alert_id`, `alert_type`, `severity_token`, `effect_token`, `alert_scope`.\n",
        "\n",
        "7. **Aggregate per (bin, route, stop)**\n",
        "\n",
        "   * Group by `timestamp_bin, route_id, stop_id` and compute:\n",
        "\n",
        "     * `incident_flag`: `nunique(alert_id)` ‚Üí then converted to binary (`>0`)\n",
        "     * `alert_type`: **mode** within the bin\n",
        "     * `alert_scope`: **mode** within the bin\n",
        "     * `sev_set`: set of severities present\n",
        "     * `eff_set`: set of effects present\n",
        "\n",
        "8. **One-hot encode**\n",
        "\n",
        "   * For severity: `INFO, WARNING, SEVERE` ‚Üí `alert_severity_*` (1 if present in `sev_set`)\n",
        "   * For effect: `NO_SERVICE, REDUCED_SERVICE, SIGNIFICANT_DELAYS, DETOUR` ‚Üí `alert_effect_*`\n",
        "\n",
        "9. **Join to your fact table**\n",
        "\n",
        "   * Left-merge the aggregated alert features onto `fact_df` by `(timestamp_bin, route_id, stop_id)`.\n",
        "   * Fill missing one-hots and `incident_flag` with 0 (int).\n",
        "\n",
        "10. **Edge case handling**\n",
        "\n",
        "    * If `alerts_df` is empty, return `fact_df` with all alert features set to 0 and `alert_scope/alert_type` as `NA`.\n",
        "\n",
        "---\n",
        "\n",
        "## Output (what you get back)\n",
        "\n",
        "A DataFrame aligned to your **fact table index** with these added columns:\n",
        "\n",
        "* `incident_flag` (0/1) ‚Äî at least one alert intersects the bin/scope\n",
        "* `alert_severity_INFO`, `alert_severity_WARNING`, `alert_severity_SEVERE` (0/1)\n",
        "* `alert_effect_NO_SERVICE`, `alert_effect_REDUCED_SERVICE`, `alert_effect_SIGNIFICANT_DELAYS`, `alert_effect_DETOUR` (0/1)\n",
        "* `alert_scope` (mode of `stop`/`route`/`system` for that bin)\n",
        "* `alert_type` (mode of `PLANNED_WORK` / `DELAYS` / `NO_SERVICE` / `GENERAL`)\n",
        "\n",
        "### Key parameters you can tune\n",
        "\n",
        "* `bin_freq=\"10min\"`: temporal granularity of alert influence\n",
        "* `horizon_hours=6`: cap for open-ended alerts without `end_ts`\n",
        "\n",
        "### Why this is useful\n",
        "\n",
        "* Produces **model-ready** binary/categorical features that explain **operational state** in each time bin.\n",
        "* Respects **spatial scope** (stop vs route vs system) and **temporal overlap**.\n",
        "* Easy to extend (e.g., more effects, richer NLP, severity weighting, windowed lags).\n"
      ],
      "metadata": {
        "id": "VA0EyHMZxp3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- inputs ---\n",
        "# alerts_df columns: alert_id, start_ts, end_ts, route_id, stop_id, header, description, cause, effect, severity_level, is_active_now\n",
        "# fact_df columns:   timestamp_bin (UTC, dt64[ns, UTC]), route_id, stop_id  (your headway/delay table index)\n",
        "\n",
        "def _alert_scope(row):\n",
        "    if pd.notna(row.stop_id):  return \"stop\"\n",
        "    if pd.notna(row.route_id): return \"route\"\n",
        "    return \"system\"  # (agency-wide/system-wide)\n",
        "\n",
        "def build_alert_features(alerts_df: pd.DataFrame,\n",
        "                         fact_df: pd.DataFrame,\n",
        "                         bin_freq=\"10min\",\n",
        "                         horizon_hours=6) -> pd.DataFrame:\n",
        "    df = alerts_df.copy()\n",
        "    if df.empty:\n",
        "        return fact_df.assign(\n",
        "            incident_flag=0,\n",
        "            alert_severity_INFO=0, alert_severity_WARNING=0, alert_severity_SEVERE=0,\n",
        "            alert_effect_NO_SERVICE=0, alert_effect_REDUCED_SERVICE=0,\n",
        "            alert_effect_SIGNIFICANT_DELAYS=0, alert_effect_DETOUR=0,\n",
        "            alert_scope=pd.NA, alert_type=pd.NA\n",
        "        )\n",
        "\n",
        "    # normalize times & scope\n",
        "    df[\"start_ts\"] = pd.to_datetime(df[\"start_ts\"], utc=True)\n",
        "    df[\"end_ts\"]   = pd.to_datetime(df[\"end_ts\"],   utc=True)\n",
        "    df[\"alert_scope\"] = df.apply(_alert_scope, axis=1)\n",
        "\n",
        "    # map severity/effect to uppercase tokens\n",
        "    df[\"severity_token\"] = df[\"severity_level\"].str.upper().fillna(\"UNKNOWN\")\n",
        "    df[\"effect_token\"]   = df[\"effect\"].str.upper().fillna(\"UNKNOWN\")\n",
        "\n",
        "    # derive a simple alert_type from header/description (customize as you like)\n",
        "    def infer_type(h, d):\n",
        "        text = f\"{h or ''} {d or ''}\".lower()\n",
        "        if \"planned work\" in text or \"maintenance\" in text: return \"PLANNED_WORK\"\n",
        "        if \"delay\" in text or \"running local\" in text:      return \"DELAYS\"\n",
        "        if \"no service\" in text or \"suspended\" in text:     return \"NO_SERVICE\"\n",
        "        return \"GENERAL\"\n",
        "    df[\"alert_type\"] = [infer_type(h, d) for h, d in zip(df[\"header\"], df[\"description\"])]\n",
        "\n",
        "    # expand alerts into 10-min bins between start and end (cap very long open-ended alerts)\n",
        "    end_cap = pd.Timestamp.utcnow().tz_localize(\"UTC\") + pd.Timedelta(hours=horizon_hours)\n",
        "    df[\"end_ts_filled\"] = df[\"end_ts\"].fillna(end_cap)\n",
        "\n",
        "    records = []\n",
        "    for idx, r in df.iterrows():\n",
        "        try:\n",
        "            bins = pd.date_range(r.start_ts.floor(bin_freq),\n",
        "                                 r.end_ts_filled.ceil(bin_freq),\n",
        "                                 freq=bin_freq, tz=\"UTC\")\n",
        "        except Exception:\n",
        "            continue\n",
        "        scope = r.alert_scope\n",
        "        # emit route-level rows\n",
        "        if scope in (\"route\", \"system\"):\n",
        "            for ts in bins:\n",
        "                records.append((ts, r.route_id if pd.notna(r.route_id) else None, None,\n",
        "                                r.alert_id, r.alert_type, r.severity_token, r.effect_token, scope))\n",
        "        # emit stop-level rows\n",
        "        if scope == \"stop\":\n",
        "            for ts in bins:\n",
        "                records.append((ts, r.route_id, r.stop_id,\n",
        "                                r.alert_id, r.alert_type, r.severity_token, r.effect_token, scope))\n",
        "\n",
        "    long = pd.DataFrame.from_records(\n",
        "        records,\n",
        "        columns=[\"timestamp_bin\",\"route_id\",\"stop_id\",\"alert_id\",\"alert_type\",\"severity\",\"effect\",\"alert_scope\"]\n",
        "    ).drop_duplicates()\n",
        "\n",
        "    # Aggregate to one row per (bin, route, stop)\n",
        "    agg = (long\n",
        "           .groupby([\"timestamp_bin\",\"route_id\",\"stop_id\"], dropna=False)\n",
        "           .agg(\n",
        "               incident_flag = (\"alert_id\", \"nunique\"),\n",
        "               alert_type    = (\"alert_type\", lambda s: s.mode().iat[0] if len(s.mode()) else None),\n",
        "               alert_scope   = (\"alert_scope\", lambda s: s.mode().iat[0] if len(s.mode()) else None),\n",
        "               sev_set       = (\"severity\", lambda s: set(s.dropna())),\n",
        "               eff_set       = (\"effect\",   lambda s: set(s.dropna())),\n",
        "           ).reset_index())\n",
        "\n",
        "    # one-hots\n",
        "    for sev in [\"INFO\",\"WARNING\",\"SEVERE\"]:\n",
        "        agg[f\"alert_severity_{sev}\"] = agg[\"sev_set\"].apply(lambda S: int(sev in S))\n",
        "    for eff in [\"NO_SERVICE\",\"REDUCED_SERVICE\",\"SIGNIFICANT_DELAYS\",\"DETOUR\"]:\n",
        "        agg[f\"alert_effect_{eff}\"] = agg[\"eff_set\"].apply(lambda S: int(eff in S))\n",
        "\n",
        "    agg[\"incident_flag\"] = (agg[\"incident_flag\"] > 0).astype(int)\n",
        "    agg = agg.drop(columns=[\"sev_set\",\"eff_set\"])\n",
        "\n",
        "    # join to fact table\n",
        "    features = fact_df.merge(\n",
        "        agg,\n",
        "        on=[\"timestamp_bin\",\"route_id\",\"stop_id\"],\n",
        "        how=\"left\",\n",
        "        suffixes=(\"\",\"\")\n",
        "    )\n",
        "\n",
        "    # fill NA where appropriate\n",
        "    for c in [c for c in features.columns if c.startswith(\"alert_severity_\") or c.startswith(\"alert_effect_\")]:\n",
        "        features[c] = features[c].fillna(0).astype(int)\n",
        "    features[\"incident_flag\"] = features[\"incident_flag\"].fillna(0).astype(int)\n",
        "\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "398Tax9crf42"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here‚Äôs the straight-shot read of what that script does.\n",
        "\n",
        "# What this code does (purpose ‚Üí method ‚Üí output)\n",
        "\n",
        "## Purpose\n",
        "\n",
        "Build a **headway fact table** for NYC Subway from the **NYCT GTFS-Realtime (nyct/gtfs)** feed, suitable for analysis and ML. It:\n",
        "\n",
        "* polls the realtime feed for a short window,\n",
        "* infers **arrival events** from **VehiclePositions**,\n",
        "* computes **headways** and rolling stats per `(route_id, stop_id, direction_id)`,\n",
        "* adds simple time features,\n",
        "* saves a **Parquet** file you can later enrich (alerts, turnstiles, weather).\n",
        "\n",
        "---\n",
        "\n",
        "## How it works (step-by-step)\n",
        "\n",
        "1. **Config**\n",
        "\n",
        "   * `DURATION_MIN` (default 20): how long to poll.\n",
        "   * `POLL_EVERY_SEC` (15): polling cadence.\n",
        "   * `BIN_FREQ` (\"10min\"): binning granularity for aggregates.\n",
        "   * `OUTPUT_PATH`: Parquet output path.\n",
        "   * Optional `MTA_API_KEY` via env var; sets `x-api-key` header if present.\n",
        "\n",
        "2. **Fetch GTFS-RT bytes**\n",
        "\n",
        "   * Tries the two canonical `nyct/gtfs` URLs (with/without `%2F`).\n",
        "   * Uses browsery headers; raises on HTTP errors.\n",
        "\n",
        "3. **Parse VehiclePositions**\n",
        "\n",
        "   * For each entity with `vehicle`:\n",
        "\n",
        "     * Extracts `route_id`, `stop_id`, `direction_id` (0/1), `vehicle_id`, `current_status`, and a UTC **timestamp** (vehicle ts; falls back to feed header ts; else `now`).\n",
        "     * Keeps only rows where all of `route_id`, `stop_id`, `direction_id`, `vehicle_id` exist.\n",
        "\n",
        "4. **Detect ‚Äúarrival events‚Äù**\n",
        "\n",
        "   * Treats an arrival when `current_status == STOPPED_AT` (GTFS-RT enum value `1`).\n",
        "   * **De-dupes** with a cooldown: `(vehicle_id, stop_id)` must be at least `ARRIVAL_COOLDOWN_SEC` (60s) apart to register another arrival (prevents repeated detections during dwell).\n",
        "   * Accumulates arrivals during the polling window, sleeping `POLL_EVERY_SEC` between polls.\n",
        "   * Returns a DataFrame of arrivals: `arrival_ts, route_id, stop_id, direction_id, vehicle_id`.\n",
        "\n",
        "5. **Compute headways & features**\n",
        "\n",
        "   * Sort by `(route_id, direction_id, stop_id, arrival_ts)`.\n",
        "   * `Hr_sec` = time since previous arrival at that stop & direction; `Hr_min` in minutes.\n",
        "   * `timestamp_bin` = `arrival_ts.floor(BIN_FREQ)` for aggregation.\n",
        "   * Rolling stats per `(route, dir, stop)`:\n",
        "\n",
        "     * `Hr_roll_mean`, `Hr_roll_std` over last 6 observations (‚âà ~1 hr if headways ~10 min).\n",
        "   * Calendar features: `hour`, `weekday`.\n",
        "   * **Crude `delay_class`** label by comparing `Hr_min` to `Hr_roll_mean`:\n",
        "\n",
        "     * `on_time` (‚â§1.25√ó), `minor_delay` (‚â§1.75√ó), `major_delay` (>1.75√ó), else `unknown`.\n",
        "     * (Meant as a placeholder until you join schedule-based labels from GTFS-Static.)\n",
        "\n",
        "6. **Aggregate to fact table**\n",
        "\n",
        "   * Group by `(timestamp_bin, route_id, stop_id, direction_id)` and take the **last** values in the bin for:\n",
        "\n",
        "     * `Hr_obs` (last headway in minutes), `Hr_roll_mean`, `Hr_roll_std`, `last_arrival_ts`, `delay_class`.\n",
        "   * Adds placeholders you‚Äôll join later:\n",
        "\n",
        "     * `entries_rate` (turnstiles), `precip_mm` (NOAA) initialized to 0.0.\n",
        "\n",
        "7. **Persist**\n",
        "\n",
        "   * Writes `fact_transit_performance.parquet` if non-empty.\n",
        "\n",
        "---\n",
        "\n",
        "## Output\n",
        "\n",
        "* **File:** `fact_transit_performance.parquet`\n",
        "* **Grain:** one row per `(timestamp_bin, route_id, stop_id, direction_id)`\n",
        "* **Key columns:**\n",
        "\n",
        "  * `Hr_obs` ‚Äî most recent headway (min) observed in the bin\n",
        "  * `Hr_roll_mean`, `Hr_roll_std` ‚Äî recent rolling stats (min)\n",
        "  * `last_arrival_ts` ‚Äî timestamp of the latest arrival in the bin\n",
        "  * `delay_class` ‚Äî coarse categorical label\n",
        "  * `hour`, `weekday` ‚Äî time features\n",
        "  * `entries_rate`, `precip_mm` ‚Äî placeholders for later joins\n",
        "\n",
        "---\n",
        "\n",
        "## Key assumptions & limitations (good to note in your capstone)\n",
        "\n",
        "* **Arrival proxy:** Uses `STOPPED_AT` vehicle status as an arrival event; doesn‚Äôt use TripUpdates‚Äô stop-times. This is robust in practice but can miss edge cases.\n",
        "* **Cooldown heuristic:** 60s filter to avoid duplicate arrivals during dwell; tune for different dwell profiles.\n",
        "* **Short polling window:** You need longer runs or repeated batches to build a rich dataset; `DURATION_MIN` is just a starter.\n",
        "* **Delay labeling:** Relative to recent observed headways, **not** schedule adherence. Replace with schedule-based labels by joining GTFS-Static `stop_times.txt` for production.\n",
        "* **Coverage:** Only routes/stops where VehiclePositions are published with `stop_id` populated will generate arrivals.\n",
        "\n",
        "---\n",
        "\n",
        "## Easy extensions (drop-in ideas)\n",
        "\n",
        "* **Join alerts:** merge the alert features you built (`incident_flag`, severity/effect one-hots) by `(timestamp_bin, route_id, stop_id)`.\n",
        "* **Weather & demand:** join NOAA precipitation/temperature and turnstile entry rates to model causal drivers.\n",
        "* **Lag features:** add lagged headways (t-1, t-2), expanding means/std over longer windows.\n",
        "* **Schedule deviation:** compute `actual_headway ‚Äì scheduled_headway` using GTFS-Static to get a real ‚Äúdelay‚Äù metric.\n"
      ],
      "metadata": {
        "id": "LnQU7_Lgykno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Build a headway fact table from MTA NYCT GTFS-RT (nyct/gtfs).\n",
        "\n",
        "Installs (once):\n",
        "  pip install gtfs-realtime-bindings pandas numpy pyarrow requests\n",
        "\n",
        "Outputs:\n",
        "  fact_transit_performance.parquet  (core headway fact table)\n",
        "\n",
        "Notes:\n",
        "  - Polls the realtime feed every 15s for DURATION_MIN minutes.\n",
        "  - Derives arrival events from VehiclePositions when current_status == STOPPED_AT.\n",
        "  - Computes headways per (route_id, stop_id, direction_id).\n",
        "  - Adds time features (hour, weekday). You can later join turnstiles/weather.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timezone\n",
        "from collections import defaultdict\n",
        "\n",
        "from google.transit import gtfs_realtime_pb2\n",
        "\n",
        "# ===============================\n",
        "# CONFIG\n",
        "# ===============================\n",
        "DURATION_MIN      = 20           # how long to poll (minutes). Increase for richer data.\n",
        "POLL_EVERY_SEC    = 15           # poll cadence\n",
        "BIN_FREQ          = \"10min\"      # time bin for aggregates\n",
        "OUTPUT_PATH       = \"fact_transit_performance.parquet\"\n",
        "\n",
        "# NYCT GTFS-RT feed (TripUpdates + VehiclePositions)\n",
        "GTFS_URLS = [\n",
        "    \"https://api-endpoint.mta.info/Dataservice/mtagtfsfeeds/nyct/gtfs\",\n",
        "    \"https://api-endpoint.mta.info/Dataservice/mtagtfsfeeds/nyct%2Fgtfs\",\n",
        "]\n",
        "\n",
        "HEADERS_BASE = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\",\n",
        "    \"Accept\": \"application/x-protobuf\",\n",
        "}\n",
        "API_KEY = os.getenv(\"MTA_API_KEY\")  # optional; some environments still require it\n",
        "\n",
        "# ===============================\n",
        "# Helpers\n",
        "# ===============================\n",
        "def fetch_gtfs_bytes():\n",
        "    headers = dict(HEADERS_BASE)\n",
        "    if API_KEY:\n",
        "        headers[\"x-api-key\"] = API_KEY\n",
        "    last_err = None\n",
        "    for url in GTFS_URLS:\n",
        "        try:\n",
        "            r = requests.get(url, headers=headers, timeout=30)\n",
        "            r.raise_for_status()\n",
        "            return r.content\n",
        "        except requests.HTTPError as e:\n",
        "            last_err = e\n",
        "            continue\n",
        "    raise last_err\n",
        "\n",
        "def parse_vehicle_positions(pb_bytes):\n",
        "    \"\"\"\n",
        "    Returns list of dict rows with:\n",
        "      ts, route_id, stop_id, direction_id, vehicle_id, current_status\n",
        "    \"\"\"\n",
        "    feed = gtfs_realtime_pb2.FeedMessage()\n",
        "    feed.ParseFromString(pb_bytes)\n",
        "\n",
        "    rows = []\n",
        "    for ent in feed.entity:\n",
        "        if not ent.HasField(\"vehicle\"):\n",
        "            continue\n",
        "        v = ent.vehicle\n",
        "\n",
        "        # Some producers put route_id in vehicle.trip.route_id, others in vehicle.vehicle.label\n",
        "        route_id = None\n",
        "        try:\n",
        "            if v.trip and v.trip.route_id:\n",
        "                route_id = v.trip.route_id\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        stop_id = None\n",
        "        try:\n",
        "            if v.stop_id:\n",
        "                stop_id = v.stop_id\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        direction_id = None\n",
        "        try:\n",
        "            if v.trip and (v.trip.direction_id in (0,1)):\n",
        "                direction_id = v.trip.direction_id\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        vehicle_id = None\n",
        "        try:\n",
        "            if v.vehicle and v.vehicle.id:\n",
        "                vehicle_id = v.vehicle.id\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # current_status: 0 = IN_TRANSIT_TO, 1 = STOPPED_AT, 2 = INCOMING_AT (per GTFS-RT enum)\n",
        "        status = None\n",
        "        try:\n",
        "            status = v.current_status\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # timestamp\n",
        "        ts = None\n",
        "        try:\n",
        "            # vehicle timestamp is epoch seconds\n",
        "            if v.timestamp:\n",
        "                ts = datetime.fromtimestamp(int(v.timestamp), tz=timezone.utc)\n",
        "        except Exception:\n",
        "            pass\n",
        "        # fallback to feed header time if missing\n",
        "        if ts is None:\n",
        "            try:\n",
        "                ts = datetime.fromtimestamp(int(feed.header.timestamp), tz=timezone.utc)\n",
        "            except Exception:\n",
        "                ts = datetime.now(timezone.utc)\n",
        "\n",
        "        if route_id and stop_id and (direction_id in (0,1)) and vehicle_id is not None:\n",
        "            rows.append({\n",
        "                \"obs_ts\": ts,\n",
        "                \"route_id\": str(route_id),\n",
        "                \"stop_id\": str(stop_id),\n",
        "                \"direction_id\": int(direction_id),\n",
        "                \"vehicle_id\": str(vehicle_id),\n",
        "                \"current_status\": int(status) if status is not None else None,\n",
        "            })\n",
        "    return rows\n",
        "\n",
        "# ===============================\n",
        "# Arrival event extraction logic\n",
        "# ===============================\n",
        "\"\"\"\n",
        "We treat an 'arrival event' when a vehicle is observed at\n",
        "current_status == STOPPED_AT at a stop. We de-dupe by (vehicle_id, stop_id)\n",
        "with a small cooldown so a long dwell doesn‚Äôt produce duplicates.\n",
        "\"\"\"\n",
        "\n",
        "STOPPED_AT = 1\n",
        "ARRIVAL_COOLDOWN_SEC = 60  # minimum gap to register another arrival for same (vehicle, stop)\n",
        "\n",
        "def accumulate_arrivals(duration_min=DURATION_MIN, poll_sec=POLL_EVERY_SEC):\n",
        "    seen_last_arrival = {}  # (vehicle_id, stop_id) -> last_arrival_ts (epoch)\n",
        "    arrivals = []           # list of dicts with arrival events\n",
        "\n",
        "    t_end = time.time() + duration_min * 60.0\n",
        "    iter_n = 0\n",
        "    while time.time() < t_end:\n",
        "        iter_n += 1\n",
        "        try:\n",
        "            pb = fetch_gtfs_bytes()\n",
        "            vp_rows = parse_vehicle_positions(pb)\n",
        "        except Exception as e:\n",
        "            # keep going on transient errors\n",
        "            time.sleep(poll_sec)\n",
        "            continue\n",
        "\n",
        "        now_epoch = time.time()\n",
        "        for r in vp_rows:\n",
        "            if r[\"current_status\"] != STOPPED_AT:\n",
        "                continue\n",
        "            key = (r[\"vehicle_id\"], r[\"stop_id\"])\n",
        "\n",
        "            last = seen_last_arrival.get(key)\n",
        "            # register new arrival if cooldown elapsed or first sighting\n",
        "            if (last is None) or ((now_epoch - last) > ARRIVAL_COOLDOWN_SEC):\n",
        "                seen_last_arrival[key] = now_epoch\n",
        "                arrivals.append({\n",
        "                    \"arrival_ts\": r[\"obs_ts\"],  # UTC-aware datetime\n",
        "                    \"route_id\": r[\"route_id\"],\n",
        "                    \"stop_id\": r[\"stop_id\"],\n",
        "                    \"direction_id\": r[\"direction_id\"],\n",
        "                    \"vehicle_id\": r[\"vehicle_id\"],\n",
        "                })\n",
        "        time.sleep(poll_sec)\n",
        "\n",
        "    return pd.DataFrame(arrivals)\n",
        "\n",
        "# ===============================\n",
        "# Build headways & fact table\n",
        "# ===============================\n",
        "def build_headways(df_arrivals: pd.DataFrame) -> pd.DataFrame:\n",
        "    if df_arrivals.empty:\n",
        "        return pd.DataFrame(columns=[\n",
        "            \"timestamp_bin\",\"route_id\",\"stop_id\",\"direction_id\",\n",
        "            \"arrival_ts\",\"Hr_sec\",\"Hr_min\",\"Hr_roll_mean\",\"Hr_roll_std\",\n",
        "            \"hour\",\"weekday\",\"delay_class\"  # placeholder label if you want to add now\n",
        "        ])\n",
        "\n",
        "    df = df_arrivals.sort_values([\"route_id\",\"direction_id\",\"stop_id\",\"arrival_ts\"]).copy()\n",
        "    # compute headway = difference between consecutive arrivals at same (route, dir, stop)\n",
        "    df[\"prev_arrival_ts\"] = df.groupby([\"route_id\",\"direction_id\",\"stop_id\"])[\"arrival_ts\"].shift(1)\n",
        "    df[\"Hr_sec\"] = (df[\"arrival_ts\"] - df[\"prev_arrival_ts\"]).dt.total_seconds()\n",
        "    df = df.dropna(subset=[\"Hr_sec\"])\n",
        "    df[\"Hr_min\"] = df[\"Hr_sec\"] / 60.0\n",
        "\n",
        "    # time bin\n",
        "    df[\"timestamp_bin\"] = df[\"arrival_ts\"].dt.floor(BIN_FREQ)\n",
        "\n",
        "    # rolling stats per stop/dir\n",
        "    df[\"Hr_roll_mean\"] = (\n",
        "        df.groupby([\"route_id\",\"direction_id\",\"stop_id\"])[\"Hr_min\"]\n",
        "          .transform(lambda s: s.rolling(window=6, min_periods=2).mean())  # ~ last ~1 hr if 10-min-ish headways\n",
        "    )\n",
        "    df[\"Hr_roll_std\"] = (\n",
        "        df.groupby([\"route_id\",\"direction_id\",\"stop_id\"])[\"Hr_min\"]\n",
        "          .transform(lambda s: s.rolling(window=6, min_periods=2).std())\n",
        "    )\n",
        "\n",
        "    # simple calendar features\n",
        "    df[\"hour\"] = df[\"arrival_ts\"].dt.hour\n",
        "    df[\"weekday\"] = df[\"arrival_ts\"].dt.weekday  # Monday=0\n",
        "\n",
        "    # OPTIONAL: create a quick categorical 'delay_class' based on headway inflation vs recent mean\n",
        "    # (For true delay vs schedule you will later join GTFS-Static stop_times)\n",
        "    def label_delay(hr, hr_mean):\n",
        "        # crude thresholds; you will replace with schedule-based labels later\n",
        "        if pd.isna(hr_mean):\n",
        "            return \"unknown\"\n",
        "        ratio = hr / (hr_mean if hr_mean > 0 else hr)\n",
        "        if ratio <= 1.25:\n",
        "            return \"on_time\"\n",
        "        elif ratio <= 1.75:\n",
        "            return \"minor_delay\"\n",
        "        else:\n",
        "            return \"major_delay\"\n",
        "    df[\"delay_class\"] = [label_delay(hr, m) for hr, m in zip(df[\"Hr_min\"], df[\"Hr_roll_mean\"])]\n",
        "\n",
        "    # We keep the granular rows (one per arrival); for modeling we aggregate to one row per bin\n",
        "    fact = (df.groupby([\"timestamp_bin\",\"route_id\",\"stop_id\",\"direction_id\"])\n",
        "              .agg(\n",
        "                  Hr_obs          = (\"Hr_min\",\"last\"),\n",
        "                  Hr_roll_mean    = (\"Hr_roll_mean\",\"last\"),\n",
        "                  Hr_roll_std     = (\"Hr_roll_std\",\"last\"),\n",
        "                  last_arrival_ts = (\"arrival_ts\",\"last\"),\n",
        "                  delay_class     = (\"delay_class\",\"last\")\n",
        "              )\n",
        "              .reset_index())\n",
        "\n",
        "    # placeholders you can fill later when you join other sources\n",
        "    fact[\"entries_rate\"] = 0.0     # join turnstiles later\n",
        "    fact[\"precip_mm\"]    = 0.0     # join NOAA later\n",
        "\n",
        "    return fact\n",
        "\n",
        "# ===============================\n",
        "# Orchestrate\n",
        "# ===============================\n",
        "def main():\n",
        "    print(f\"Polling nyct/gtfs for {DURATION_MIN} minutes‚Ä¶\")\n",
        "    arrivals = accumulate_arrivals(DURATION_MIN, POLL_EVERY_SEC)\n",
        "    print(f\"Collected arrival events: {len(arrivals):,}\")\n",
        "\n",
        "    fact = build_headways(arrivals)\n",
        "    print(f\"Headway fact rows: {len(fact):,}\")\n",
        "\n",
        "    # Save\n",
        "    if not fact.empty:\n",
        "        fact.to_parquet(OUTPUT_PATH, index=False)\n",
        "        print(f\"Wrote: {OUTPUT_PATH}\")\n",
        "    else:\n",
        "        print(\"No data collected. Increase DURATION_MIN and try again.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "6TJxzd_juVKK",
        "outputId": "b3fc2c0e-a1c1-4ea8-cf44-547ce55e0cba"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polling nyct/gtfs for 20 minutes‚Ä¶\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1669359831.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1669359831.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Polling nyct/gtfs for {DURATION_MIN} minutes‚Ä¶\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m     \u001b[0marrivals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccumulate_arrivals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDURATION_MIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPOLL_EVERY_SEC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Collected arrival events: {len(arrivals):,}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1669359831.py\u001b[0m in \u001b[0;36maccumulate_arrivals\u001b[0;34m(duration_min, poll_sec)\u001b[0m\n\u001b[1;32m    185\u001b[0m                     \u001b[0;34m\"vehicle_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vehicle_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 })\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrivals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X = numerical + alert features\n",
        "alert_cols = [\n",
        "  \"incident_flag\",\"alert_severity_INFO\",\"alert_severity_WARNING\",\"alert_severity_SEVERE\",\n",
        "  \"alert_effect_NO_SERVICE\",\"alert_effect_REDUCED_SERVICE\",\"alert_effect_SIGNIFICANT_DELAYS\",\"alert_effect_DETOUR\"\n",
        "]\n",
        "num_cols = [\"Hr_roll_mean\",\"Hr_roll_std\",\"entries_rate\",\"precip_mm\",\"hour\",\"weekday\"]  # example\n",
        "X = features[num_cols + alert_cols]\n",
        "y = features[\"delay_class\"]  # or Hr_next/EWT_pw_sec\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "clf = XGBClassifier(\n",
        "    n_estimators=500, max_depth=8, learning_rate=0.05,\n",
        "    subsample=0.9, colsample_bytree=0.8, eval_metric=\"mlogloss\", random_state=42\n",
        ")\n",
        "clf.fit(X, y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "IU6sf2lHrpRu",
        "outputId": "fd547014-e0ea-46b3-ec7f-b646d24e4c5e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'features' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3429171878.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m ]\n\u001b[1;32m      6\u001b[0m \u001b[0mnum_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Hr_roll_mean\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Hr_roll_std\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"entries_rate\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"precip_mm\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"hour\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"weekday\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_cols\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malert_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"delay_class\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# or Hr_next/EWT_pw_sec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = features[\"incident_flag\"] == 1\n",
        "print(\"Alert windows F1:\", f1_score(y[mask], clf.predict(X[mask]), average=\"macro\"))\n",
        "print(\"Normal windows F1:\", f1_score(y[~mask], clf.predict(X[~mask]), average=\"macro\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "-6teU4VsrvMw",
        "outputId": "c39302a1-f42b-4b95-f138-40cb6aaf7905"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'features' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2880422480.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"incident_flag\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Alert windows F1:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"macro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Normal windows F1:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"macro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'features' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Merged Dataset: Headway Fact Table √ó Alert Features**\n",
        "\n",
        "This process creates a unified, modeling-ready dataset that combines operational headway data with service alert indicators.\n",
        "Below is an overview of what this step does and what the resulting dataset looks like.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Purpose**\n",
        "\n",
        "* Integrates train performance data (headways, delays, and timing features) with corresponding MTA service alert data.\n",
        "* Produces a single dataset where each record represents a specific ten-minute time bin for a given route and stop.\n",
        "* Enables modeling and analysis of how different alert types and severities influence train delays.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Method**\n",
        "\n",
        "* Both datasets‚Äô timestamps are standardized to UTC to ensure alignment across time bins.\n",
        "* A **left join** is performed on the keys:\n",
        "\n",
        "  * **timestamp_bin** (the 10-minute interval)\n",
        "  * **route_id** (the subway line)\n",
        "  * **stop_id** (the affected station)\n",
        "* Missing alert fields are replaced with default values:\n",
        "\n",
        "  * Numeric flags ‚Üí 0 (meaning no alert)\n",
        "  * Text categories ‚Üí ‚Äúnone‚Äù (meaning not applicable)\n",
        "* A quick summary prints the total number of bins joined and how many contained active alerts.\n",
        "* The merged dataset is saved as **fact_with_alerts.parquet** for later use in modeling.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Resulting Columns**\n",
        "\n",
        "* **timestamp_bin** ‚Äì start time of the 10-minute interval (UTC)\n",
        "* **route_id** ‚Äì subway line identifier (e.g., ‚ÄúA‚Äù, ‚Äú6‚Äù, ‚ÄúD‚Äù)\n",
        "* **stop_id** ‚Äì station identifier (e.g., ‚ÄúR14‚Äù)\n",
        "* **Hr_obs** ‚Äì most recent observed headway (minutes)\n",
        "* **Hr_roll_mean**, **Hr_roll_std** ‚Äì recent rolling mean and standard deviation of headway\n",
        "* **delay_class** ‚Äì categorical label: on_time, minor_delay, major_delay\n",
        "* **incident_flag** ‚Äì 1 if an alert was active during the bin; 0 otherwise\n",
        "* **alert_severity_INFO**, **alert_severity_WARNING**, **alert_severity_SEVERE** ‚Äì one-hot indicators for severity level\n",
        "* **alert_effect_NO_SERVICE**, **alert_effect_REDUCED_SERVICE**, **alert_effect_SIGNIFICANT_DELAYS**, **alert_effect_DETOUR** ‚Äì one-hot indicators for operational impact\n",
        "* **alert_scope** ‚Äì level of impact (stop, route, or system)\n",
        "* **alert_type** ‚Äì general classification (DELAYS, PLANNED_WORK, NO_SERVICE, or GENERAL)\n",
        "\n",
        "---\n",
        "\n",
        "#### **Sample Output (illustrative)**\n",
        "\n",
        "| timestamp_bin        | route_id | stop_id | Hr_obs | delay_class | incident_flag | alert_severity_SEVERE | alert_effect_SIGNIFICANT_DELAYS | alert_scope | alert_type   |\n",
        "| -------------------- | -------- | ------- | ------ | ----------- | ------------- | --------------------- | ------------------------------- | ----------- | ------------ |\n",
        "| 2025-10-15 10:00 UTC | D        | R14     | 9.2    | on_time     | 0             | 0                     | 0                               | none        | none         |\n",
        "| 2025-10-15 10:10 UTC | D        | R14     | 15.8   | minor_delay | 1             | 1                     | 1                               | route       | DELAYS       |\n",
        "| 2025-10-15 10:20 UTC | D        | R14     | 21.5   | major_delay | 1             | 0                     | 1                               | route       | PLANNED_WORK |\n",
        "\n",
        "---\n",
        "\n",
        "#### **Outcome**\n",
        "\n",
        "* The merged table now holds both **performance metrics** and **alert indicators**.\n",
        "* It serves as the foundation for **supervised learning** or **causal analysis** linking service disruptions to train delay patterns.\n",
        "\n",
        "\n",
        "### **Merged Dataset: Headway Fact Table √ó Alert Features**\n",
        "\n",
        "This step produces a single, modeling-ready dataset that fuses the train-performance metrics from your headway fact table with alert indicators from the GTFS-Realtime alerts feed.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Purpose**\n",
        "\n",
        "* Combine operational headway and delay metrics with service-alert information for each ten-minute time bin.\n",
        "* Provide a complete view of how route- or stop-level alerts coincide with observed headway performance.\n",
        "* Prepare a unified table for statistical analysis or machine-learning models.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Method**\n",
        "\n",
        "* Convert timestamps in both tables to UTC to align bins.\n",
        "* Perform a **left join** on the shared keys:\n",
        "  ‚Ä¢ `timestamp_bin`‚ÄÉ‚Ä¢ `route_id`‚ÄÉ‚Ä¢ `stop_id`\n",
        "* Fill missing alert values with defaults so periods without alerts are explicitly coded:\n",
        "  ‚Ä¢ numeric flags ‚Üí 0‚ÄÉ‚Ä¢ text categories ‚Üí ‚Äúnone‚Äù.\n",
        "* Print a brief summary confirming dataset size and number of bins with active alerts.\n",
        "* Save the merged result as **fact_with_alerts.parquet** for downstream use.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Key Columns**\n",
        "\n",
        "* **timestamp_bin** ‚Äì start of the ten-minute interval (UTC)\n",
        "* **route_id** ‚Äì subway line identifier (e.g., A, 6, D)\n",
        "* **stop_id** ‚Äì station identifier\n",
        "* **Hr_obs**, **Hr_roll_mean**, **Hr_roll_std** ‚Äì observed and rolling headways (minutes)\n",
        "* **delay_class** ‚Äì categorical label: on_time / minor_delay / major_delay\n",
        "* **incident_flag** ‚Äì 1 if any alert active, 0 otherwise\n",
        "* **alert_severity_INFO**, **_WARNING**, **_SEVERE** ‚Äì severity one-hot flags\n",
        "* **alert_effect_NO_SERVICE**, **_REDUCED_SERVICE**, **_SIGNIFICANT_DELAYS**, **_DETOUR** ‚Äì effect one-hot flags\n",
        "* **alert_scope** ‚Äì impact level (stop / route / system / none)\n",
        "* **alert_type** ‚Äì classified alert category (DELAYS, PLANNED_WORK, NO_SERVICE, GENERAL, none)\n",
        "\n",
        "---\n",
        "\n",
        "#### **Sample Output (illustrative)**\n",
        "\n",
        "| timestamp_bin        | route_id | stop_id | Hr_obs | delay_class | incident_flag | alert_severity_SEVERE | alert_effect_SIGNIFICANT_DELAYS | alert_scope | alert_type   |\n",
        "| -------------------- | -------- | ------- | ------ | ----------- | ------------- | --------------------- | ------------------------------- | ----------- | ------------ |\n",
        "| 2025-10-15 10:00 UTC | D        | R14     | 9.2    | on_time     | 0             | 0                     | 0                               | none        | none         |\n",
        "| 2025-10-15 10:10 UTC | D        | R14     | 15.8   | minor_delay | 1             | 1                     | 1                               | route       | DELAYS       |\n",
        "| 2025-10-15 10:20 UTC | D        | R14     | 21.5   | major_delay | 1             | 0                     | 1                               | route       | PLANNED_WORK |\n",
        "\n",
        "---\n",
        "\n",
        "#### **Outcome**\n",
        "\n",
        "* Each record now includes both **performance** and **alert context**, forming the foundation for modeling the relationship between service disruptions and train delay behavior.\n"
      ],
      "metadata": {
        "id": "aTrgdXJe0CRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Merge: Headway Fact Table  √ó  Alert Features\n",
        "# ============================================================\n",
        "\n",
        "# --- Inputs expected ---\n",
        "# fact_df   ‚Üí output from build_headways()\n",
        "# alerts_df ‚Üí output from build_alert_features()\n",
        "\n",
        "# Ensure timestamps are aligned and UTC-normalized\n",
        "fact_df[\"timestamp_bin\"] = pd.to_datetime(fact_df[\"timestamp_bin\"], utc=True)\n",
        "alerts_df[\"timestamp_bin\"] = pd.to_datetime(alerts_df[\"timestamp_bin\"], utc=True)\n",
        "\n",
        "# --- Join operation (left join retains all headway rows) ---\n",
        "merged_df = fact_df.merge(\n",
        "    alerts_df[\n",
        "        [\n",
        "            \"timestamp_bin\", \"route_id\", \"stop_id\",\n",
        "            \"incident_flag\",\n",
        "            \"alert_severity_INFO\", \"alert_severity_WARNING\", \"alert_severity_SEVERE\",\n",
        "            \"alert_effect_NO_SERVICE\", \"alert_effect_REDUCED_SERVICE\",\n",
        "            \"alert_effect_SIGNIFICANT_DELAYS\", \"alert_effect_DETOUR\",\n",
        "            \"alert_scope\", \"alert_type\"\n",
        "        ]\n",
        "    ],\n",
        "    on=[\"timestamp_bin\", \"route_id\", \"stop_id\"],\n",
        "    how=\"left\",\n",
        "    suffixes=(\"\", \"_alert\")\n",
        ")\n",
        "\n",
        "# --- Fill missing alert features with defaults (no alert observed) ---\n",
        "for c in [\n",
        "    \"incident_flag\",\n",
        "    \"alert_severity_INFO\", \"alert_severity_WARNING\", \"alert_severity_SEVERE\",\n",
        "    \"alert_effect_NO_SERVICE\", \"alert_effect_REDUCED_SERVICE\",\n",
        "    \"alert_effect_SIGNIFICANT_DELAYS\", \"alert_effect_DETOUR\"\n",
        "]:\n",
        "    merged_df[c] = merged_df[c].fillna(0).astype(int)\n",
        "\n",
        "merged_df[\"alert_scope\"] = merged_df[\"alert_scope\"].fillna(\"none\")\n",
        "merged_df[\"alert_type\"]  = merged_df[\"alert_type\"].fillna(\"none\")\n",
        "\n",
        "# --- Optional sanity check ---\n",
        "print(\"Merged dataset shape:\", merged_df.shape)\n",
        "print(\"Alerts joined on:\", merged_df[['incident_flag']].sum().item(), \"bins with active alerts\")\n",
        "\n",
        "# --- Save merged dataset ---\n",
        "merged_df.to_parquet(\"fact_with_alerts.parquet\", index=False)\n",
        "print(\"‚úÖ fact_with_alerts.parquet written successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "TWYPWMn_zMGe",
        "outputId": "39d231d6-a076-4bf8-bb58-0154b2aabddb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'fact_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-828090830.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Ensure timestamps are aligned and UTC-normalized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfact_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timestamp_bin\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfact_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timestamp_bin\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0malerts_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timestamp_bin\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malerts_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timestamp_bin\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fact_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gtfs_static_reader.py\n",
        "# ------------------------------------------------------------\n",
        "# Read Static GTFS (local ZIP), resolve service active on a date,\n",
        "# build timezone-aware scheduled stop times, and compute headways.\n",
        "#\n",
        "# Usage (example at bottom):\n",
        "#   gtfs = read_gtfs_zip(\"nyct_gtfs_static.zip\")\n",
        "#   svc = active_service_ids(gtfs, \"2025-10-15\")\n",
        "#   sched = build_scheduled_stop_times(gtfs, \"2025-10-15\", tz=\"America/New_York\")\n",
        "#   hw = scheduled_headways_by_stop_route_dir(sched, bin_freq=\"10min\")\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import io\n",
        "import zipfile\n",
        "from datetime import datetime, date, timedelta\n",
        "from zoneinfo import ZoneInfo\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Core loaders\n",
        "# ---------------------------\n",
        "def read_gtfs_zip(path_zip: str, tables=None) -> dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Load selected GTFS tables from a local .zip into pandas DataFrames.\n",
        "    Returns a dict like {\"stops\": df, \"trips\": df, ...}.\n",
        "    \"\"\"\n",
        "    default_tables = [\n",
        "        \"agency\", \"routes\", \"trips\", \"stops\",\n",
        "        \"stop_times\", \"calendar\", \"calendar_dates\", \"shapes\"\n",
        "    ]\n",
        "    want = set(tables or default_tables)\n",
        "\n",
        "    dfs = {}\n",
        "    with zipfile.ZipFile(path_zip, \"r\") as zf:\n",
        "        names_in_zip = {n.lower(): n for n in zf.namelist()}\n",
        "        for t in want:\n",
        "            fname = f\"{t}.txt\"\n",
        "            if fname not in names_in_zip and fname.lower() in names_in_zip:\n",
        "                fname = names_in_zip[fname.lower()]\n",
        "            if fname not in names_in_zip:\n",
        "                # Not all feeds include all files (e.g., calendar or shapes may be missing)\n",
        "                continue\n",
        "            with zf.open(fname) as f:\n",
        "                # Read as UTF-8 with dtype inference; you can add specific dtypes if needed.\n",
        "                dfs[t] = pd.read_csv(io.TextIOWrapper(f, encoding=\"utf-8\"))\n",
        "    return dfs\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Service calendar resolution\n",
        "# ---------------------------\n",
        "def _to_ymd(d) -> date:\n",
        "    if isinstance(d, str):\n",
        "        return datetime.strptime(d, \"%Y-%m-%d\").date()\n",
        "    if isinstance(d, datetime):\n",
        "        return d.date()\n",
        "    return d  # assume date\n",
        "\n",
        "\n",
        "def _dow_mask(target: date) -> str:\n",
        "    # GTFS calendar columns are monday..sunday (1/0)\n",
        "    return [\"monday\",\"tuesday\",\"wednesday\",\"thursday\",\"friday\",\"saturday\",\"sunday\"][target.weekday()]\n",
        "\n",
        "\n",
        "def active_service_ids(gtfs: dict, when, *, include_exceptions=True) -> set[str]:\n",
        "    \"\"\"\n",
        "    Return the set of service_id that run on 'when' (YYYY-MM-DD|date|datetime).\n",
        "    Uses calendar + calendar_dates (exceptions).\n",
        "    \"\"\"\n",
        "    d = _to_ymd(when)\n",
        "\n",
        "    base = set()\n",
        "    cal = gtfs.get(\"calendar\")\n",
        "    if cal is not None and not cal.empty:\n",
        "        # filter date range and day-of-week\n",
        "        cal = cal.copy()\n",
        "        cal[\"start_date\"] = pd.to_datetime(cal[\"start_date\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
        "        cal[\"end_date\"]   = pd.to_datetime(cal[\"end_date\"],   format=\"%Y%m%d\", errors=\"coerce\")\n",
        "        mask = (\n",
        "            (cal[\"start_date\"].dt.date <= d) &\n",
        "            (cal[\"end_date\"].dt.date   >= d) &\n",
        "            (cal[_dow_mask(d)] == 1)\n",
        "        )\n",
        "        base = set(cal.loc[mask, \"service_id\"].astype(str))\n",
        "\n",
        "    if include_exceptions:\n",
        "        cdx = gtfs.get(\"calendar_dates\")\n",
        "        if cdx is not None and not cdx.empty:\n",
        "            cdx = cdx.copy()\n",
        "            cdx[\"date\"] = pd.to_datetime(cdx[\"date\"], format=\"%Y%m%d\", errors=\"coerce\").dt.date\n",
        "            # exception_type: 1 = service added; 2 = service removed\n",
        "            added  = set(cdx.loc[(cdx[\"date\"] == d) & (cdx[\"exception_type\"] == 1), \"service_id\"].astype(str))\n",
        "            removed= set(cdx.loc[(cdx[\"date\"] == d) & (cdx[\"exception_type\"] == 2), \"service_id\"].astype(str))\n",
        "            base = (base | added) - removed\n",
        "\n",
        "    return base\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Time parsing helpers\n",
        "# ---------------------------\n",
        "def parse_gtfs_hhmmss_to_seconds(s: str) -> int | None:\n",
        "    \"\"\"\n",
        "    GTFS times can exceed 24:00:00 (e.g., 27:15:00).\n",
        "    Return seconds from service-day midnight. None for invalid.\n",
        "    \"\"\"\n",
        "    if pd.isna(s):\n",
        "        return None\n",
        "    try:\n",
        "        parts = str(s).split(\":\")\n",
        "        if len(parts) != 3:\n",
        "            return None\n",
        "        h, m, sec = int(parts[0]), int(parts[1]), int(parts[2])\n",
        "        return h*3600 + m*60 + sec\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def seconds_to_datetime_local(base_date: date, seconds_from_midnight: int, tz: str) -> datetime:\n",
        "    \"\"\"\n",
        "    Convert seconds since service-day midnight (possibly > 24h) into\n",
        "    a timezone-aware datetime in local time.\n",
        "    \"\"\"\n",
        "    base_dt = datetime.combine(base_date, datetime.min.time()).replace(tzinfo=ZoneInfo(tz))\n",
        "    return base_dt + timedelta(seconds=int(seconds_from_midnight))\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Build scheduled stop times\n",
        "# ---------------------------\n",
        "def build_scheduled_stop_times(\n",
        "    gtfs: dict,\n",
        "    service_date,\n",
        "    tz: str = \"America/New_York\",\n",
        "    include_departure=False,\n",
        "    keep_columns_extra=None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Return a DataFrame of scheduled stop events for the given service_date with:\n",
        "      ['service_date','route_id','trip_id','stop_id','stop_sequence','direction_id',\n",
        "       'arrival_sec','arrival_time','departure_sec','departure_time','shape_id', ...]\n",
        "    Times are timezone-aware datetimes in 'tz'.\n",
        "    \"\"\"\n",
        "    d = _to_ymd(service_date)\n",
        "    svc_ids = active_service_ids(gtfs, d)\n",
        "    if not svc_ids:\n",
        "        return pd.DataFrame(columns=[\n",
        "            \"service_date\",\"route_id\",\"trip_id\",\"stop_id\",\"stop_sequence\",\"direction_id\",\n",
        "            \"arrival_sec\",\"arrival_time\",\"departure_sec\",\"departure_time\",\"shape_id\"\n",
        "        ])\n",
        "\n",
        "    trips = gtfs.get(\"trips\", pd.DataFrame()).copy()\n",
        "    stop_times = gtfs.get(\"stop_times\", pd.DataFrame()).copy()\n",
        "    routes = gtfs.get(\"routes\", pd.DataFrame()).copy()\n",
        "\n",
        "    # Filter trips by active service_ids\n",
        "    trips = trips[trips[\"service_id\"].astype(str).isin(svc_ids)].copy()\n",
        "    if trips.empty or stop_times.empty:\n",
        "        return pd.DataFrame(columns=[\n",
        "            \"service_date\",\"route_id\",\"trip_id\",\"stop_id\",\"stop_sequence\",\"direction_id\",\n",
        "            \"arrival_sec\",\"arrival_time\",\"departure_sec\",\"departure_time\",\"shape_id\"\n",
        "        ])\n",
        "\n",
        "    # Join stop_times ‚Üî trips ‚Üî routes (route_id, direction_id, shape_id)\n",
        "    st = stop_times.merge(\n",
        "        trips[[\"trip_id\",\"route_id\",\"direction_id\",\"shape_id\"]].astype({\"trip_id\":\"string\"}),\n",
        "        on=\"trip_id\", how=\"inner\"\n",
        "    ).merge(\n",
        "        routes[[\"route_id\"]].astype({\"route_id\":\"string\"}),\n",
        "        on=\"route_id\", how=\"left\"\n",
        "    )\n",
        "\n",
        "    # Parse HH:MM:SS ‚Üí seconds from midnight (can exceed 86400)\n",
        "    st[\"arrival_sec\"]   = st[\"arrival_time\"].apply(parse_gtfs_hhmmss_to_seconds)\n",
        "    st[\"departure_sec\"] = st[\"departure_time\"].apply(parse_gtfs_hhmmss_to_seconds)\n",
        "\n",
        "    # Build timezone-aware datetimes\n",
        "    st[\"arrival_time\"]   = st[\"arrival_sec\"].apply(lambda s: seconds_to_datetime_local(d, s, tz) if pd.notna(s) else pd.NaT)\n",
        "    st[\"departure_time\"] = st[\"departure_sec\"].apply(lambda s: seconds_to_datetime_local(d, s, tz) if pd.notna(s) else pd.NaT)\n",
        "\n",
        "    # Clean types\n",
        "    st[\"stop_sequence\"] = pd.to_numeric(st[\"stop_sequence\"], errors=\"coerce\").astype(\"Int64\")\n",
        "    st[\"direction_id\"]  = pd.to_numeric(st[\"direction_id\"], errors=\"coerce\").astype(\"Int8\")\n",
        "    st[\"service_date\"]  = pd.Timestamp(d).tz_localize(ZoneInfo(tz))\n",
        "\n",
        "    cols = [\n",
        "        \"service_date\",\"route_id\",\"trip_id\",\"stop_id\",\"stop_sequence\",\"direction_id\",\n",
        "        \"arrival_sec\",\"arrival_time\"\n",
        "    ]\n",
        "    if include_departure:\n",
        "        cols += [\"departure_sec\",\"departure_time\"]\n",
        "    if \"shape_id\" in st.columns:\n",
        "        cols += [\"shape_id\"]\n",
        "    if keep_columns_extra:\n",
        "        cols += [c for c in keep_columns_extra if c in st.columns]\n",
        "\n",
        "    st = st[cols].sort_values([\"route_id\",\"direction_id\",\"stop_id\",\"arrival_time\"])\n",
        "    return st.reset_index(drop=True)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Scheduled headways (optional)\n",
        "# ---------------------------\n",
        "def scheduled_headways_by_stop_route_dir(sched_df: pd.DataFrame, bin_freq=\"10min\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute scheduled headways at each (route_id, stop_id, direction_id).\n",
        "    Returns an aggregated fact table by time bin:\n",
        "      ['timestamp_bin','route_id','stop_id','direction_id','Sch_Hr_min']\n",
        "    \"\"\"\n",
        "    if sched_df.empty:\n",
        "        return pd.DataFrame(columns=[\n",
        "            \"timestamp_bin\",\"route_id\",\"stop_id\",\"direction_id\",\"Sch_Hr_min\"\n",
        "        ])\n",
        "\n",
        "    df = sched_df.dropna(subset=[\"arrival_time\"]).copy()\n",
        "    df[\"arrival_time\"] = pd.to_datetime(df[\"arrival_time\"], utc=False).dt.tz_convert(\"UTC\")\n",
        "    df = df.sort_values([\"route_id\",\"direction_id\",\"stop_id\",\"arrival_time\"])\n",
        "\n",
        "    # Headway in minutes between consecutive scheduled arrivals at the same stop/direction\n",
        "    df[\"prev_arrival\"] = df.groupby([\"route_id\",\"direction_id\",\"stop_id\"])[\"arrival_time\"].shift(1)\n",
        "    df[\"Sch_Hr_min\"] = (df[\"arrival_time\"] - df[\"prev_arrival\"]).dt.total_seconds() / 60.0\n",
        "    df = df.dropna(subset=[\"Sch_Hr_min\"])\n",
        "\n",
        "    # Bin on UTC to align with your realtime facts (which you stored in UTC bins)\n",
        "    df[\"timestamp_bin\"] = df[\"arrival_time\"].dt.tz_convert(\"UTC\").dt.floor(bin_freq)\n",
        "\n",
        "    fact = (df.groupby([\"timestamp_bin\",\"route_id\",\"stop_id\",\"direction_id\"])\n",
        "              .agg(Sch_Hr_min=(\"Sch_Hr_min\",\"last\"))\n",
        "              .reset_index())\n",
        "\n",
        "    return fact\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Example usage (uncomment and edit path/date)\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    # 1) Point to your local GTFS static ZIP\n",
        "    GTFS_ZIP = \"nyct_gtfs_static.zip\"  # e.g., downloaded from your MTA developer portal\n",
        "\n",
        "    # 2) Load core tables\n",
        "    gtfs = read_gtfs_zip(GTFS_ZIP)\n",
        "\n",
        "    # 3) Pick a service date (NYC time)\n",
        "    service_date = \"2025-10-15\"\n",
        "\n",
        "    # 4) Build scheduled stop times with TZ-aware datetimes\n",
        "    sched = build_scheduled_stop_times(gtfs, service_date, tz=\"America/New_York\", include_departure=True)\n",
        "    print(\"Scheduled rows:\", len(sched))\n",
        "    print(sched.head(6))\n",
        "\n",
        "    # 5) (Optional) Compute scheduled headways per stop/route/direction, binned to 10 minutes\n",
        "    sch_hw = scheduled_headways_by_stop_route_dir(sched, bin_freq=\"10min\")\n",
        "    print(\"Scheduled headway fact rows:\", len(sch_hw))\n",
        "    print(sch_hw.head(6))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "83irWi4v2W6-",
        "outputId": "7992074f-7d72-4ec0-c5f4-6ff1627bb845"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'nyct_gtfs_static.zip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2848601393.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;31m# 2) Load core tables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m     \u001b[0mgtfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_gtfs_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGTFS_ZIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;31m# 3) Pick a service date (NYC time)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2848601393.py\u001b[0m in \u001b[0;36mread_gtfs_zip\u001b[0;34m(path_zip, tables)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_zip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mnames_in_zip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwant\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/zipfile/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'nyct_gtfs_static.zip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import zipfile\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# URL of the live MTA Subway GTFS (static)\n",
        "GTFS_URL = \"https://rrgtfsfeeds.s3.amazonaws.com/gtfs_subway.zip\"\n",
        "\n",
        "def read_gtfs_from_url(url):\n",
        "    r = requests.get(url, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    zf = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "    gtfs_tables = {}\n",
        "    for name in zf.namelist():\n",
        "        if name.endswith(\".txt\"):\n",
        "            with zf.open(name) as f:\n",
        "                gtfs_tables[name.replace(\".txt\", \"\")] = pd.read_csv(f)\n",
        "    return gtfs_tables\n",
        "\n",
        "# Example: load and inspect\n",
        "gtfs = read_gtfs_from_url(GTFS_URL)\n",
        "print(gtfs.keys())\n",
        "print(gtfs[\"routes\"].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2Fmtr8I4hRp",
        "outputId": "a29dc992-995e-449c-8de2-85de5d5c42ed"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['agency', 'calendar_dates', 'calendar', 'routes', 'shapes', 'stop_times', 'stops', 'transfers', 'trips'])\n",
            "  route_id agency_id route_short_name            route_long_name  \\\n",
            "0        1  MTA NYCT                1  Broadway - 7 Avenue Local   \n",
            "1        2  MTA NYCT                2           7 Avenue Express   \n",
            "2        3  MTA NYCT                3           7 Avenue Express   \n",
            "3        4  MTA NYCT                4   Lexington Avenue Express   \n",
            "4        5  MTA NYCT                5   Lexington Avenue Express   \n",
            "\n",
            "                                          route_desc  route_type  \\\n",
            "0  Trains operate between 242 St in the Bronx and...           1   \n",
            "1  Trains operate between Wakefield-241 St, Bronx...           1   \n",
            "2  Trains operate between 148 St, Manhattan, and ...           1   \n",
            "3  Trains operate daily between Woodlawn/Jerome A...           1   \n",
            "4  Weekdays daytime, most trains operate between ...           1   \n",
            "\n",
            "                                        route_url route_color route_text_color  \n",
            "0  http://web.mta.info/nyct/service/pdf/t1cur.pdf      EE352E              NaN  \n",
            "1  http://web.mta.info/nyct/service/pdf/t2cur.pdf      EE352E                   \n",
            "2  http://web.mta.info/nyct/service/pdf/t3cur.pdf      EE352E              NaN  \n",
            "3  http://web.mta.info/nyct/service/pdf/t4cur.pdf      00933C              NaN  \n",
            "4  http://web.mta.info/nyct/service/pdf/t5cur.pdf      00933C              NaN  \n"
          ]
        }
      ]
    }
  ]
}