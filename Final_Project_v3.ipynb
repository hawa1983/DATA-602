{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hawa1983/DATA-602/blob/main/Final_Project_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OptAJ12iBc8G"
      },
      "source": [
        "### **Purpose**\n",
        "\n",
        "The purpose of this script is to enrich the MovieLens movie dataset (`movies.dat`) with detailed movie metadata from The Movie Database (TMDB) API. This metadata includes movie overviews, genres, poster and backdrop image URLs, cast and director information, keywords, user ratings, and trailer links. The enriched dataset will serve as the foundation for building content-based, collaborative, and hybrid recommender systems.\n",
        "\n",
        "### **Methodology**\n",
        "\n",
        "1. **Load MovieLens Movie Data**\n",
        "   The script loads the `movies.dat` file, which contains basic movie information including `movieId`, `title`, and `genres`.\n",
        "\n",
        "2. **Clean Titles and Extract Years**\n",
        "   It processes the movie titles to remove the year from the title string and separately extracts the release year to improve search accuracy when querying TMDB.\n",
        "\n",
        "3. **Query TMDB API**\n",
        "   For each movie, it sends a search request to TMDB using the cleaned title and release year. If a match is found, it retrieves the movie’s TMDB ID.\n",
        "\n",
        "4. **Retrieve Detailed Metadata**\n",
        "   Using the TMDB ID, the script fetches:\n",
        "\n",
        "   * Overview (plot summary)\n",
        "   * Poster and backdrop image paths\n",
        "   * Genre IDs, which are then mapped to readable genre names\n",
        "   * Top 3 cast members\n",
        "   * Director(s)\n",
        "   * Associated keywords\n",
        "   * YouTube trailer link (if available)\n",
        "\n",
        "5. **Construct and Save Enriched Dataset**\n",
        "   All metadata is compiled into a structured format and merged with the original MovieLens data. The final dataset is saved as `movies_enriched_full.csv` for downstream use in recommendation models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qJ0maYGaxtJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# ---------------------------------------\n",
        "# CONFIG\n",
        "# ---------------------------------------\n",
        "BASE_URL = \"https://api.themoviedb.org/3\"\n",
        "IMAGE_BASE = \"https://image.tmdb.org/t/p/w500\"\n",
        "\n",
        "# Use your TMDB Bearer Token (v4)\n",
        "HEADERS = {\n",
        "    \"Authorization\": \"Bearer eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiIyZGZlNjMwMGMzYjIzMjc2NzExNjQ0N2JhNzhiMjM5MyIsIm5iZiI6MTc1MTkyMjA3Ni4xMzUsInN1YiI6IjY4NmMzNTljMzc4NjllOGEyNDUxZTM0OSIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.S773ddH3FiIHtokPW4sYpJog0mXWS1o4OPov1KZneUw\"\n",
        "}\n",
        "\n",
        "# TMDB genre ID to name mapping\n",
        "GENRE_ID_TO_NAME = {\n",
        "    28: \"Action\", 12: \"Adventure\", 16: \"Animation\", 35: \"Comedy\", 80: \"Crime\",\n",
        "    99: \"Documentary\", 18: \"Drama\", 10751: \"Family\", 14: \"Fantasy\", 36: \"History\",\n",
        "    27: \"Horror\", 10402: \"Music\", 9648: \"Mystery\", 10749: \"Romance\", 878: \"Science Fiction\",\n",
        "    10770: \"TV Movie\", 53: \"Thriller\", 10752: \"War\", 37: \"Western\"\n",
        "}\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 1: Load MovieLens .dat Files\n",
        "# ---------------------------------------\n",
        "\n",
        "# Load movies.dat - format: MovieID::Title::Genres\n",
        "movies_df = pd.read_csv(\"movies.dat\", sep=\"::\", engine='python', header=None, names=[\"movieId\", \"title\", \"genres\"], encoding=\"latin-1\")\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 2: Clean Movie Titles and Extract Year\n",
        "# ---------------------------------------\n",
        "\n",
        "def extract_year(title):\n",
        "    if \"(\" in title:\n",
        "        try:\n",
        "            return int(title.strip()[-5:-1])\n",
        "        except:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def clean_title(title):\n",
        "    if \"(\" in title:\n",
        "        return title[:title.rfind(\"(\")].strip()\n",
        "    return title.strip()\n",
        "\n",
        "movies_df[\"year\"] = movies_df[\"title\"].apply(extract_year)\n",
        "movies_df[\"clean_title\"] = movies_df[\"title\"].apply(clean_title)\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 3: TMDB Metadata Functions\n",
        "# ---------------------------------------\n",
        "\n",
        "# Search for movie in TMDB\n",
        "def search_tmdb(title, year):\n",
        "    url = f\"{BASE_URL}/search/movie\"\n",
        "    params = {\"query\": title, \"year\": year}\n",
        "    response = requests.get(url, headers=HEADERS, params=params)\n",
        "    r = response.json()\n",
        "    if r.get(\"results\"):\n",
        "        return r[\"results\"][0]\n",
        "    return None\n",
        "\n",
        "# Get full metadata from TMDB\n",
        "def get_full_tmdb_metadata(tmdb_id):\n",
        "    metadata = {}\n",
        "\n",
        "    # Credits (cast, crew)\n",
        "    credits = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/credits\", headers=HEADERS).json()\n",
        "    cast = [c[\"name\"] for c in credits.get(\"cast\", [])[:3]]\n",
        "    directors = [c[\"name\"] for c in credits.get(\"crew\", []) if c.get(\"job\") == \"Director\"]\n",
        "\n",
        "    # Keywords\n",
        "    keywords = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/keywords\", headers=HEADERS).json()\n",
        "    keyword_list = [k[\"name\"] for k in keywords.get(\"keywords\", [])]\n",
        "\n",
        "    # Videos (trailers)\n",
        "    videos = requests.get(f\"{BASE_URL}/movie/{tmdb_id}/videos\", headers=HEADERS).json()\n",
        "    trailer_links = [\n",
        "        f\"https://www.youtube.com/watch?v={v['key']}\"\n",
        "        for v in videos.get(\"results\", [])\n",
        "        if v[\"site\"] == \"YouTube\" and v[\"type\"] == \"Trailer\"\n",
        "    ]\n",
        "\n",
        "    # Final metadata dictionary\n",
        "    metadata[\"top_3_cast\"] = \", \".join(cast)\n",
        "    metadata[\"directors\"] = \", \".join(directors)\n",
        "    metadata[\"keywords\"] = \", \".join(keyword_list)\n",
        "    metadata[\"trailer_link\"] = trailer_links[0] if trailer_links else None\n",
        "\n",
        "    return metadata\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 4: Enrich Movie Data\n",
        "# ---------------------------------------\n",
        "\n",
        "enriched = []\n",
        "\n",
        "for _, row in tqdm(movies_df.iterrows(), total=len(movies_df)):\n",
        "    movie_data = search_tmdb(row[\"clean_title\"], row[\"year\"])\n",
        "\n",
        "    if movie_data:\n",
        "        tmdb_id = movie_data[\"id\"]\n",
        "        extra = get_full_tmdb_metadata(tmdb_id)\n",
        "\n",
        "        genre_ids = movie_data.get(\"genre_ids\", [])\n",
        "        genre_names = [GENRE_ID_TO_NAME.get(gid, str(gid)) for gid in genre_ids]\n",
        "\n",
        "        enriched.append({\n",
        "            \"tmdb_id\": tmdb_id,\n",
        "            \"overview\": movie_data.get(\"overview\", \"\"),\n",
        "            \"poster_path\": IMAGE_BASE + movie_data.get(\"poster_path\", \"\") if movie_data.get(\"poster_path\") else None,\n",
        "            \"backdrop_path\": IMAGE_BASE + movie_data.get(\"backdrop_path\", \"\") if movie_data.get(\"backdrop_path\") else None,\n",
        "            \"vote_average\": movie_data.get(\"vote_average\", None),\n",
        "            \"vote_count\": movie_data.get(\"vote_count\", None),\n",
        "            \"tmdb_genres\": \", \".join(genre_names),\n",
        "            **extra\n",
        "        })\n",
        "    else:\n",
        "        enriched.append({\n",
        "            \"tmdb_id\": None,\n",
        "            \"overview\": None,\n",
        "            \"poster_path\": None,\n",
        "            \"backdrop_path\": None,\n",
        "            \"vote_average\": None,\n",
        "            \"vote_count\": None,\n",
        "            \"tmdb_genres\": None,\n",
        "            \"top_3_cast\": None,\n",
        "            \"directors\": None,\n",
        "            \"keywords\": None,\n",
        "            \"trailer_link\": None\n",
        "        })\n",
        "\n",
        "    time.sleep(0.25)  # Respect TMDB API rate limits\n",
        "\n",
        "# ---------------------------------------\n",
        "# STEP 5: Save Final Dataset\n",
        "# ---------------------------------------\n",
        "\n",
        "enriched_df = pd.DataFrame(enriched)\n",
        "final_df = pd.concat([movies_df, enriched_df], axis=1)\n",
        "final_df.to_csv(\"movies_enriched_full.csv\", index=False)\n",
        "\n",
        "print(\"DONE: Saved as 'movies_enriched_full.csv'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7KNZkOThu7F"
      },
      "source": [
        "## **Personalized Content-Based Movie Recommendation System**\n",
        "\n",
        "This Python script implements a **Content-Based Filtering (CBF)** system enhanced with **personalized recommendations** using user-specific rating profiles. Built using the MovieLens 1M dataset and enriched metadata, the pipeline performs vectorization, similarity computation, and profile-based predictions.\n",
        "\n",
        "**What This Script Does**\n",
        "\n",
        "* **Module 1–2**: Load essential libraries and enriched movie data.\n",
        "* **Module 3**: Load user ratings and demographics.\n",
        "* **Module 4**: Engineer features combining genres, cast, crew, keywords, and movie overviews.\n",
        "* **Module 5**: Transform content into TF-IDF, Count, or Binary vectors, and compute pairwise similarities using Cosine or Jaccard metrics.\n",
        "* **Module 6**: Construct a weighted content profile per user based on past ratings.\n",
        "* **Module 7**: Recommend top-N movies similar to the user profile, excluding already seen titles.\n",
        "\n",
        "**Techniques Used**\n",
        "\n",
        "* **Text Vectorization**: TF-IDF, CountVectorizer, Binary Count\n",
        "* **Similarity Metrics**: Cosine Similarity, Jaccard Similarity\n",
        "* **Personalization**: Weighted vector averaging based on each user’s rated items\n",
        "* **Parallelization**: Speeds up Jaccard similarity computation using joblib\n",
        "\n",
        "**Use Cases**\n",
        "\n",
        "* Personalized recommendations for new users with a few ratings (cold-start)\n",
        "* Improving diversity and relevance in suggested movies\n",
        "* Generating fallback content suggestions in hybrid recommender systems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9GeB2GWMyfPL",
        "outputId": "0dd43904-a19e-45df-bec4-e69bf7799e0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4 (from -r requirements.txt (line 1))\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m466.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.2.2)\n",
            "Collecting scikit-surprise==1.1.4 (from -r requirements.txt (line 3))\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-learn==1.4.2 (from -r requirements.txt (line 4))\n",
            "  Downloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting matplotlib==3.8.4 (from -r requirements.txt (line 5))\n",
            "  Downloading matplotlib-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: seaborn==0.13.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.13.2)\n",
            "Collecting tqdm==4.66.4 (from -r requirements.txt (line 7))\n",
            "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib==1.5.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (1.5.1)\n",
            "Requirement already satisfied: pyspark==3.5.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (0.10.9.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2->-r requirements.txt (line 2)) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise==1.1.4->-r requirements.txt (line 3)) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.2->-r requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.8.4->-r requirements.txt (line 5)) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2->-r requirements.txt (line 2)) (1.17.0)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.4.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp311-cp311-linux_x86_64.whl size=2469542 sha256=b7824633188ffbc8cb3cd5fed396940484b1d606ec3a0a183e54f0db45649c18\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/8f/6e/7e2899163e2d85d8266daab4aa1cdabec7a6c56f83c015b5af\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: tqdm, numpy, scikit-surprise, scikit-learn, matplotlib\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cuml-cu12 25.6.0 requires scikit-learn>=1.5, but you have scikit-learn 1.4.2 which is incompatible.\n",
            "dataproc-spark-connect 0.8.2 requires tqdm>=4.67, but you have tqdm 4.66.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.4.2 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed matplotlib-3.8.4 numpy-1.26.4 scikit-learn-1.4.2 scikit-surprise-1.1.4 tqdm-4.66.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              },
              "id": "17d063935acc43bf8b541998d25912c8"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj82RTA_ldyR"
      },
      "source": [
        "### **Personalized Content-Based Movie Recommendation System Using Hybrid Textual Metadata and Multiple Similarity Models**\n",
        "\n",
        "**Purpose**\n",
        "\n",
        "The goal of this project is to build a personalized movie recommendation system that leverages content-based filtering techniques using enriched movie metadata. By incorporating user rating data and multiple text-based similarity strategies, the system aims to generate relevant and diverse movie suggestions tailored to individual user preferences—especially in cold-start or sparsely rated scenarios.\n",
        "\n",
        "**Methodology**\n",
        "\n",
        "1. **Data Loading & Preparation**\n",
        "\n",
        "   * Movie metadata is loaded from an enriched dataset containing genres, keywords, cast, director, overview, and release year.\n",
        "   * User ratings and demographic data are loaded and used to personalize recommendations.\n",
        "\n",
        "2. **Feature Engineering**\n",
        "\n",
        "   * A composite text field (`cbf_features`) is created for each movie by concatenating cleaned metadata fields: genres, keywords, cast, director, overview, and year.\n",
        "\n",
        "3. **Vectorization**\n",
        "\n",
        "   * Three representations of movie content are generated:\n",
        "\n",
        "     * **TF-IDF Vectors**: Capture term importance within documents.\n",
        "     * **Count Vectors**: Raw term frequencies without weighting.\n",
        "     * **Binary Genre-Like Vectors**: For Jaccard similarity (1 if feature exists).\n",
        "\n",
        "4. **Similarity Computation**\n",
        "\n",
        "   * Cosine similarity is computed for TF-IDF and Count vectors.\n",
        "   * Jaccard similarity is computed for binary vectors using pairwise intersection-over-union.\n",
        "\n",
        "5. **User Profiling & Recommendation**\n",
        "\n",
        "   * For **TF-IDF** and **Count** models:\n",
        "\n",
        "     * A personalized **user profile vector** is created using a weighted average of vectors from rated movies.\n",
        "     * Recommendations are generated by finding unseen movies most similar to the user’s profile.\n",
        "   * For the **Binary + Jaccard** model:\n",
        "\n",
        "     * The average Jaccard similarity is computed between each unseen movie and the user’s seen movies.\n",
        "\n",
        "6. **Result Generation & Tagging**\n",
        "\n",
        "   * Top 50 movie recommendations are produced per user for each model.\n",
        "   * Each output is tagged with the model name: `\"TF-IDF + Cosine\"`, `\"Count + Cosine\"`, or `\"Binary + Jaccard\"`.\n",
        "\n",
        "7. **Output Consolidation**\n",
        "\n",
        "   * All recommendation outputs are combined into one labeled DataFrame for comparative analysis and visualization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Hybrid CBF Pipeline with Genre Similarity Features for Meta-Learner\n",
        "# ===============================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# === Load Data ===\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "\n",
        "# === Feature Engineering ===\n",
        "def create_feature_string(df):\n",
        "    def clean(col):\n",
        "        return df[col].fillna('').astype(str).str.replace(',', ' ').str.replace(r'\\s+', ' ', regex=True)\n",
        "    df['cbf_features'] = (\n",
        "        clean('tmdb_genres') + ' ' +\n",
        "        clean('keywords') + ' ' +\n",
        "        clean('top_3_cast') + ' ' +\n",
        "        clean('directors') + ' ' +\n",
        "        df['overview'].fillna('').str.lower().str.replace(r'[^\\w\\s]', '', regex=True) + ' ' +\n",
        "        df['year'].astype(str)\n",
        "    )\n",
        "    return df\n",
        "\n",
        "movies = create_feature_string(movies)\n",
        "\n",
        "# === Genre Distribution Skew ===\n",
        "movies['genre_count'] = movies['tmdb_genres'].fillna('').apply(lambda x: len(x.split(',')))\n",
        "\n",
        "# === Train-Test Split Per User ===\n",
        "def train_test_split_user(ratings, test_size=0.2):\n",
        "    train_rows, test_rows = [], []\n",
        "    for user_id, group in ratings.groupby('userId'):\n",
        "        if len(group) >= 5:\n",
        "            train, test = train_test_split(group, test_size=test_size, random_state=42)\n",
        "            train_rows.append(train)\n",
        "            test_rows.append(test)\n",
        "        else:\n",
        "            train_rows.append(group)\n",
        "    return pd.concat(train_rows), pd.concat(test_rows)\n",
        "\n",
        "train_ratings, test_ratings = train_test_split_user(ratings)\n",
        "\n",
        "# === Bias Terms ===\n",
        "global_mean = train_ratings['rating'].mean()\n",
        "user_bias = train_ratings.groupby('userId')['rating'].mean() - global_mean\n",
        "item_bias = train_ratings.groupby('movieId')['rating'].mean() - global_mean\n",
        "\n",
        "# === Vectorizers ===\n",
        "tfidf_matrix = TfidfVectorizer(stop_words='english').fit_transform(movies['cbf_features'])\n",
        "count_matrix = CountVectorizer(stop_words='english').fit_transform(movies['cbf_features'])\n",
        "binary_matrix = CountVectorizer(binary=True).fit_transform(movies['cbf_features'])\n",
        "\n",
        "# === Helper Functions ===\n",
        "def build_user_profile(user_id, train_ratings, matrix, movies):\n",
        "    user_train = train_ratings[train_ratings['userId'] == user_id]\n",
        "    indices = movies[movies['movieId'].isin(user_train['movieId'])].index\n",
        "    if len(indices) == 0:\n",
        "        return None\n",
        "    weights = user_train.set_index('movieId').loc[movies.iloc[indices]['movieId']]['rating'].values\n",
        "    row_vectors = matrix[indices].toarray() if hasattr(matrix, \"toarray\") else matrix[indices]\n",
        "    return np.average(row_vectors, axis=0, weights=weights).reshape(1, -1)\n",
        "\n",
        "def save_predictions(user_ids, matrix, sim_fn, label):\n",
        "    dfs = []\n",
        "    for user_id in tqdm(user_ids, desc=f\"Scoring {label}\"):\n",
        "        profile = build_user_profile(user_id, train_ratings, matrix, movies)\n",
        "        if profile is None:\n",
        "            continue\n",
        "        user_test = test_ratings[test_ratings['userId'] == user_id]\n",
        "        test_movies = movies[movies['movieId'].isin(user_test['movieId'])]\n",
        "        test_indices = test_movies.index\n",
        "        if len(test_indices) == 0:\n",
        "            continue\n",
        "        sims = sim_fn(profile, matrix[test_indices]).flatten()\n",
        "        b_u = user_bias.get(user_id, 0)\n",
        "        b_i = item_bias.reindex(test_movies['movieId']).fillna(0).values\n",
        "        preds = np.clip(global_mean + b_u + b_i + sims * 1.5, 0.5, 5.0)\n",
        "        actual = user_test.set_index('movieId').loc[test_movies['movieId']]['rating'].values\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "            'userId': user_id,\n",
        "            'movieId': test_movies['movieId'].values,\n",
        "            'true_rating': actual,\n",
        "            f'{label}_score': preds,\n",
        "            f'{label}_genre_similarity': sims,\n",
        "            'genre_count': test_movies['genre_count'].values\n",
        "        })\n",
        "        dfs.append(df)\n",
        "    result = pd.concat(dfs)\n",
        "    result.to_csv(f'cbf_predictions_{label}.csv', index=False)\n",
        "\n",
        "# === Save All Predictions ===\n",
        "save_predictions(test_ratings['userId'].unique(), tfidf_matrix, cosine_similarity, 'tfidf')\n",
        "save_predictions(test_ratings['userId'].unique(), count_matrix, cosine_similarity, 'count')\n",
        "save_predictions(test_ratings['userId'].unique(), binary_matrix.toarray(),\n",
        "                 lambda x, y: 1 - pairwise_distances(x, y, metric='jaccard'), 'binary')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFoNOlR6jJ_0",
        "outputId": "245be7a8-0fae-4e50-a3c2-2e30644617b1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scoring tfidf: 100%|██████████| 6040/6040 [03:02<00:00, 33.14it/s]\n",
            "Scoring count: 100%|██████████| 6040/6040 [02:54<00:00, 34.68it/s]\n",
            "Scoring binary: 100%|██████████| 6040/6040 [03:21<00:00, 29.91it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2U48IdMw0rJ",
        "outputId": "b84ebf08-fb35-41d8-9490-2dbed78db0a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating users: 100%|██████████| 6040/6040 [02:57<00:00, 34.08it/s]\n",
            "Evaluating users: 100%|██████████| 6040/6040 [02:50<00:00, 35.35it/s]\n",
            "Evaluating users: 100%|██████████| 6040/6040 [03:14<00:00, 31.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF + Cosine RMSE: 0.9424\n",
            "Count + Cosine RMSE: 0.9693\n",
            "Binary + Jaccard RMSE: 0.9297\n",
            "\n",
            "Top-N Recommendations for User 5549 — TF-IDF\n",
            "      movieId                                              title  \\\n",
            "3164     3233                               Smashing Time (1967)   \n",
            "1396     1420  Message to Love: The Isle of Wight Festival (1...   \n",
            "3313     3382                             Song of Freedom (1936)   \n",
            "3587     3656                                       Lured (1947)   \n",
            "777       787                 Gate of Heavenly Peace, The (1995)   \n",
            "\n",
            "      predicted_rating  \n",
            "3164               5.0  \n",
            "1396               5.0  \n",
            "3313               5.0  \n",
            "3587               5.0  \n",
            "777                5.0  \n",
            "\n",
            "Top-N Recommendations for User 5549 — Count\n",
            "      movieId                                              title  \\\n",
            "1762     1830                            Follow the Bitch (1998)   \n",
            "3811     3881                           Bittersweet Motel (2000)   \n",
            "977       989          Schlafes Bruder (Brother of Sleep) (1995)   \n",
            "3164     3233                               Smashing Time (1967)   \n",
            "1339     1360  Identification of a Woman (Identificazione di ...   \n",
            "\n",
            "      predicted_rating  \n",
            "1762               5.0  \n",
            "3811               5.0  \n",
            "977                5.0  \n",
            "3164               5.0  \n",
            "1339               5.0  \n",
            "\n",
            "Top-N Recommendations for User 5549 — Jaccard\n",
            "\n",
            "Top-N Recommendations for User 5549 — Jaccard\n",
            "      movieId                                              title  \\\n",
            "3313     3382                             Song of Freedom (1936)   \n",
            "1762     1830                            Follow the Bitch (1998)   \n",
            "777       787                 Gate of Heavenly Peace, The (1995)   \n",
            "3164     3233                               Smashing Time (1967)   \n",
            "1396     1420  Message to Love: The Isle of Wight Festival (1...   \n",
            "\n",
            "      predicted_rating  \n",
            "3313               5.0  \n",
            "1762               5.0  \n",
            "777                5.0  \n",
            "3164               5.0  \n",
            "1396               5.0  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scoring tfidf: 100%|██████████| 6040/6040 [02:51<00:00, 35.18it/s]\n",
            "Scoring count: 100%|██████████| 6040/6040 [02:47<00:00, 36.06it/s]\n",
            "Scoring binary: 100%|██████████| 6040/6040 [03:14<00:00, 31.01it/s]\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# Hybrid CBF Pipeline with RMSE, Top-N, and CSV Export\n",
        "# ===============================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load Data\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "\n",
        "# Feature Engineering\n",
        "def create_feature_string(df):\n",
        "    def clean(col):\n",
        "        return df[col].fillna('').astype(str).str.replace(',', ' ').str.replace(r'\\s+', ' ', regex=True)\n",
        "    df['cbf_features'] = (\n",
        "        clean('tmdb_genres') + ' ' +\n",
        "        clean('keywords') + ' ' +\n",
        "        clean('top_3_cast') + ' ' +\n",
        "        clean('directors') + ' ' +\n",
        "        df['overview'].fillna('').str.lower().str.replace(r'[^\\w\\s]', '', regex=True) + ' ' +\n",
        "        df['year'].astype(str)\n",
        "    )\n",
        "    return df\n",
        "\n",
        "movies = create_feature_string(movies)\n",
        "\n",
        "# Train-Test Split Per User\n",
        "def train_test_split_user(ratings, test_size=0.2):\n",
        "    train_rows, test_rows = [], []\n",
        "    for user_id, group in ratings.groupby('userId'):\n",
        "        if len(group) >= 5:\n",
        "            train, test = train_test_split(group, test_size=test_size, random_state=42)\n",
        "            train_rows.append(train)\n",
        "            test_rows.append(test)\n",
        "        else:\n",
        "            train_rows.append(group)\n",
        "    return pd.concat(train_rows), pd.concat(test_rows)\n",
        "\n",
        "train_ratings, test_ratings = train_test_split_user(ratings)\n",
        "\n",
        "# Bias Terms\n",
        "global_mean = train_ratings['rating'].mean()\n",
        "user_bias = train_ratings.groupby('userId')['rating'].mean() - global_mean\n",
        "item_bias = train_ratings.groupby('movieId')['rating'].mean() - global_mean\n",
        "\n",
        "# Vectorizers\n",
        "tfidf_matrix = TfidfVectorizer(stop_words='english').fit_transform(movies['cbf_features'])\n",
        "count_matrix = CountVectorizer(stop_words='english').fit_transform(movies['cbf_features'])\n",
        "binary_matrix = CountVectorizer(binary=True).fit_transform(movies['cbf_features'])\n",
        "\n",
        "# Helper Functions\n",
        "def build_user_profile(user_id, train_ratings, matrix, movies):\n",
        "    user_train = train_ratings[train_ratings['userId'] == user_id]\n",
        "    indices = movies[movies['movieId'].isin(user_train['movieId'])].index\n",
        "\n",
        "    if len(indices) == 0:\n",
        "        return None\n",
        "\n",
        "    weights = user_train.set_index('movieId').loc[movies.iloc[indices]['movieId']]['rating'].values\n",
        "\n",
        "    # Check if matrix is sparse\n",
        "    row_vectors = matrix[indices].toarray() if hasattr(matrix, \"toarray\") else matrix[indices]\n",
        "\n",
        "    return np.average(row_vectors, axis=0, weights=weights).reshape(1, -1)\n",
        "\n",
        "def evaluate_rmse_for_user(user_id, train_ratings, test_ratings, matrix, movies, sim_fn):\n",
        "    profile = build_user_profile(user_id, train_ratings, matrix, movies)\n",
        "    if profile is None:\n",
        "        return None\n",
        "    user_test = test_ratings[test_ratings['userId'] == user_id]\n",
        "    test_movies = movies[movies['movieId'].isin(user_test['movieId'])]\n",
        "    test_indices = test_movies.index\n",
        "    if len(test_indices) == 0:\n",
        "        return None\n",
        "    sims = sim_fn(profile, matrix[test_indices]).flatten()\n",
        "    b_u = user_bias.get(user_id, 0)\n",
        "    b_i = item_bias.reindex(test_movies['movieId']).fillna(0).values\n",
        "    preds = np.clip(global_mean + b_u + b_i + sims * 1.5, 0.5, 5.0)\n",
        "    actual = user_test.set_index('movieId').loc[test_movies['movieId']]['rating'].values\n",
        "    return np.sqrt(mean_squared_error(actual, preds))\n",
        "\n",
        "def evaluate_rmse_all_users(train_ratings, test_ratings, matrix, movies, sim_fn):\n",
        "    user_ids = test_ratings['userId'].unique()\n",
        "    rmses = []\n",
        "    for user_id in tqdm(user_ids, desc=\"Evaluating users\"):\n",
        "        rmse = evaluate_rmse_for_user(user_id, train_ratings, test_ratings, matrix, movies, sim_fn)\n",
        "        if rmse is not None:\n",
        "            rmses.append(rmse)\n",
        "    return np.mean(rmses)\n",
        "\n",
        "def recommend_top_n(user_id, train_ratings, matrix, movies, sim_fn, top_n=50):\n",
        "    profile = build_user_profile(user_id, train_ratings, matrix, movies)\n",
        "    if profile is None:\n",
        "        return pd.DataFrame()\n",
        "    seen = train_ratings[train_ratings['userId'] == user_id]['movieId']\n",
        "    unseen = movies[~movies['movieId'].isin(seen)]\n",
        "    sims = sim_fn(profile, matrix[unseen.index]).flatten()\n",
        "    content_scores = sims * 1.5\n",
        "    b_u = user_bias.get(user_id, 0)\n",
        "    b_i = item_bias.reindex(unseen['movieId']).fillna(0).values\n",
        "    preds = np.clip(global_mean + b_u + b_i + content_scores, 0.5, 5.0)\n",
        "    top_idx = np.argsort(preds)[-top_n:][::-1]\n",
        "    return unseen.iloc[top_idx][['movieId', 'title']].assign(predicted_rating=preds[top_idx])\n",
        "\n",
        "# Save Predictions for Meta-Learner\n",
        "def save_predictions(user_ids, matrix, sim_fn, label):\n",
        "    dfs = []\n",
        "    for user_id in tqdm(user_ids, desc=f\"Scoring {label}\"):\n",
        "        profile = build_user_profile(user_id, train_ratings, matrix, movies)\n",
        "        if profile is None:\n",
        "            continue\n",
        "        user_test = test_ratings[test_ratings['userId'] == user_id]\n",
        "        test_movies = movies[movies['movieId'].isin(user_test['movieId'])]\n",
        "        test_indices = test_movies.index\n",
        "        if len(test_indices) == 0:\n",
        "            continue\n",
        "        sims = sim_fn(profile, matrix[test_indices]).flatten()\n",
        "        b_u = user_bias.get(user_id, 0)\n",
        "        b_i = item_bias.reindex(test_movies['movieId']).fillna(0).values\n",
        "        preds = np.clip(global_mean + b_u + b_i + sims * 1.5, 0.5, 5.0)\n",
        "        actual = user_test.set_index('movieId').loc[test_movies['movieId']]['rating'].values\n",
        "        df = pd.DataFrame({\n",
        "            'userId': user_id,\n",
        "            'movieId': test_movies['movieId'].values,\n",
        "            'true_rating': actual,\n",
        "            f'{label}_score': preds\n",
        "        })\n",
        "        dfs.append(df)\n",
        "    result = pd.concat(dfs)\n",
        "    result.to_csv(f'cbf_predictions_{label}.csv', index=False)\n",
        "\n",
        "# Run Evaluations and Save Predictions\n",
        "rmse_tfidf = evaluate_rmse_all_users(train_ratings, test_ratings, tfidf_matrix, movies, cosine_similarity)\n",
        "rmse_count = evaluate_rmse_all_users(train_ratings, test_ratings, count_matrix, movies, cosine_similarity)\n",
        "rmse_binary = evaluate_rmse_all_users(train_ratings, test_ratings, binary_matrix.toarray(), movies,\n",
        "                                      lambda x, y: 1 - pairwise_distances(x, y, metric='jaccard'))\n",
        "\n",
        "print(f\"\\nTF-IDF + Cosine RMSE: {rmse_tfidf:.4f}\")\n",
        "print(f\"Count + Cosine RMSE: {rmse_count:.4f}\")\n",
        "print(f\"Binary + Jaccard RMSE: {rmse_binary:.4f}\")\n",
        "\n",
        "# Top-N Recommendations for User 5549\n",
        "print(\"\\nTop-N Recommendations for User 5549 — TF-IDF\")\n",
        "print(recommend_top_n(5549, train_ratings, tfidf_matrix, movies, cosine_similarity).head())\n",
        "\n",
        "print(\"\\nTop-N Recommendations for User 5549 — Count\")\n",
        "print(recommend_top_n(5549, train_ratings, count_matrix, movies, cosine_similarity).head())\n",
        "\n",
        "print(\"\\nTop-N Recommendations for User 5549 — Jaccard\")\n",
        "print(\"\\nTop-N Recommendations for User 5549 — Jaccard\")\n",
        "print(recommend_top_n(\n",
        "    5549,\n",
        "    train_ratings,\n",
        "    binary_matrix.toarray(),   # Convert to dense\n",
        "    movies,\n",
        "    lambda x, y: 1 - pairwise_distances(x, y, metric='jaccard')\n",
        ").head())\n",
        "\n",
        "\n",
        "# Save Predictions\n",
        "save_predictions(test_ratings['userId'].unique(), tfidf_matrix, cosine_similarity, 'tfidf')\n",
        "save_predictions(test_ratings['userId'].unique(), count_matrix, cosine_similarity, 'count')\n",
        "save_predictions(test_ratings['userId'].unique(), binary_matrix.toarray(),\n",
        "                 lambda x, y: 1 - pairwise_distances(x, y, metric='jaccard'), 'binary')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omg4Y6k5XmSn"
      },
      "source": [
        "### **Memory-based collaborative filtering module (UBCF, IBCF)**\n",
        "\n",
        "***Purpose:***\n",
        "\n",
        "This module implements **memory-based collaborative filtering** using **user-user** or **item-item** similarity. It addresses **user bias** by normalizing ratings through mean-centering and optionally **rescaling predictions** to the original rating scale for interpretability.\n",
        "\n",
        "***Methodology:***\n",
        "\n",
        "1. **Rating Matrix Construction**:\n",
        "\n",
        "   * A user-item matrix is built from raw MovieLens-style ratings data.\n",
        "   * For `kind='user'`, ratings are mean-centered per user to reduce bias from lenient or strict raters.\n",
        "   * For `kind='item'`, raw ratings are used directly (no normalization), as the algorithm focuses on item similarities based on a single user's input.\n",
        "\n",
        "2. **Similarity Computation**:\n",
        "\n",
        "   * Cosine similarity is computed either:\n",
        "\n",
        "     * **Across users** for user-based CF (`kind='user'`)\n",
        "     * **Across items** for item-based CF (`kind='item'`)\n",
        "   * `sklearn.metrics.pairwise_distances` is used to derive similarity as `1 - cosine_distance`.\n",
        "\n",
        "3. **Prediction Generation**:\n",
        "\n",
        "   * For **user-based CF**:\n",
        "\n",
        "     * Ratings from similar users are weighted by similarity and averaged.\n",
        "     * The user’s mean rating is **added back** to restore predictions to the original scale (e.g., 1–5).\n",
        "   * For **item-based CF**:\n",
        "\n",
        "     * A user’s own ratings are used to compute scores for similar items.\n",
        "     * No mean is added back, since predictions are already on the correct scale.\n",
        "\n",
        "4. **Top-N Recommendations**:\n",
        "\n",
        "   * The system filters out movies the user has already rated.\n",
        "   * It ranks unseen movies by predicted score and returns the top-N recommendations.\n",
        "   * Each recommendation is labeled with the model type (`User-Based CF` or `Item-Based CF`) for downstream tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDxfs5nRH3hv",
        "outputId": "1d652fb7-702a-43b8-ea95-eee758e37051"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User-Based CF RMSE: 1.0336\n",
            "Item-Based CF RMSE: 0.8796\n",
            "Dummy Predictor RMSE: 1.1197\n",
            "\n",
            "Top 10 User-Based CF Recommendations:\n",
            "   movieId                          title     score\n",
            "0     2701          Wild Wild West (1999)  3.608871\n",
            "1     1917              Armageddon (1998)  3.608796\n",
            "2     1721                 Titanic (1997)  3.607812\n",
            "3     3753            Patriot, The (2000)  3.605514\n",
            "4     2881         Double Jeopardy (1999)  3.604429\n",
            "5     2722           Deep Blue Sea (1999)  3.602775\n",
            "6      736                 Twister (1996)  3.602065\n",
            "7     3113             End of Days (1999)  3.601182\n",
            "8      780  Independence Day (ID4) (1996)  3.600492\n",
            "9     1101                 Top Gun (1986)  3.600417\n",
            "\n",
            "Top 10 Item-Based CF Recommendations:\n",
            "   movieId                                              title     score\n",
            "0      557                                  Mamma Roma (1962)  5.000000\n",
            "1      657                                 Yankee Zulu (1994)  5.000000\n",
            "2     3601                        Castaway Cowboy, The (1974)  5.000000\n",
            "3      989          Schlafes Bruder (Brother of Sleep) (1995)  5.000000\n",
            "4     3517                                  Bells, The (1926)  5.000000\n",
            "5      729  Institute Benjamenta, or This Dream People Cal...  5.000000\n",
            "6     2591  Jeanne and the Perfect Guy (Jeanne et le garço...  5.000000\n",
            "7     1787  Paralyzing Fear: The Story of Polio in America...  4.865259\n",
            "8     3323                              Chain of Fools (2000)  4.825758\n",
            "9      787                 Gate of Heavenly Peace, The (1995)  4.823364\n",
            "✅ Saved meta-user features to: meta_user_features.csv\n",
            "✅ Saved meta-movie features to: meta_movie_features.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import datetime\n",
        "\n",
        "# === Step 1: Load Data ===\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")[['movieId', 'title']]\n",
        "\n",
        "train_ratings, test_ratings = train_test_split(ratings, test_size=0.2, random_state=42)\n",
        "\n",
        "# === Step 2: Create Bias-Adjusted Matrix ===\n",
        "def create_bias_adjusted_matrix(ratings_df):\n",
        "    matrix = ratings_df.pivot(index='userId', columns='movieId', values='rating')\n",
        "    global_mean = ratings_df['rating'].mean()\n",
        "    user_bias = matrix.sub(global_mean, axis=0).mean(axis=1)\n",
        "    item_bias = matrix.sub(global_mean, axis=0).sub(user_bias, axis=0).mean(axis=0)\n",
        "    adjusted = matrix.sub(global_mean).sub(user_bias, axis=0).sub(item_bias, axis=1)\n",
        "    return adjusted.fillna(0), global_mean, user_bias, item_bias\n",
        "\n",
        "user_item_matrix, global_mean, user_bias, item_bias = create_bias_adjusted_matrix(train_ratings)\n",
        "\n",
        "# === Step 3: Similarity Matrices ===\n",
        "user_sim_matrix = cosine_similarity(user_item_matrix)\n",
        "item_sim_matrix = cosine_similarity(user_item_matrix.T)\n",
        "\n",
        "# === Step 4: Recommender Function ===\n",
        "def recommend_memory_based(user_id, matrix, global_mean, user_bias, item_bias, sim_matrix, kind='user', top_n=50, return_full=False):\n",
        "    if kind == 'user':\n",
        "        sim_scores = sim_matrix[matrix.index.get_loc(user_id)]\n",
        "        weighted = sim_scores @ matrix.values\n",
        "        norm = np.abs(sim_scores).sum()\n",
        "        preds = weighted / norm if norm != 0 else np.zeros_like(weighted)\n",
        "        preds += global_mean + user_bias.loc[user_id]\n",
        "    else:\n",
        "        user_vector = matrix.loc[user_id]\n",
        "        weighted = user_vector @ sim_matrix\n",
        "        norm = (user_vector != 0) @ np.abs(sim_matrix)\n",
        "        with np.errstate(divide='ignore', invalid='ignore'):\n",
        "            preds = np.true_divide(weighted, norm)\n",
        "            preds[norm == 0] = 0\n",
        "        preds += global_mean + user_bias.loc[user_id] + item_bias.values\n",
        "\n",
        "    preds = np.clip(preds, 1.0, 5.0)\n",
        "    pred_series = pd.Series(preds, index=matrix.columns)\n",
        "    seen = train_ratings[train_ratings['userId'] == user_id]['movieId'].tolist()\n",
        "    pred_series = pred_series.drop(labels=seen, errors='ignore')\n",
        "\n",
        "    if return_full:\n",
        "        return pred_series\n",
        "    else:\n",
        "        top_preds = pred_series.sort_values(ascending=False).head(top_n)\n",
        "        return pd.DataFrame({\n",
        "            'userId': user_id,\n",
        "            'movieId': top_preds.index,\n",
        "            'score': top_preds.values\n",
        "        })\n",
        "\n",
        "# === Step 5: Evaluation Function ===\n",
        "def evaluate_model_and_save(test_df, matrix, global_mean, user_bias, item_bias, sim_matrix, kind='user', output_file=None):\n",
        "    all_preds = []\n",
        "    for uid in test_df['userId'].unique():\n",
        "        if uid not in matrix.index:\n",
        "            continue\n",
        "        recs = recommend_memory_based(uid, matrix, global_mean, user_bias, item_bias, sim_matrix, kind, top_n=1000, return_full=True)\n",
        "        actual = test_df[test_df['userId'] == uid]\n",
        "        merged = pd.merge(actual, recs.rename(\"score\"), on=\"movieId\")\n",
        "        all_preds.append(merged)\n",
        "\n",
        "    all_preds_df = pd.concat(all_preds, ignore_index=True)\n",
        "    if output_file:\n",
        "        all_preds_df.to_csv(output_file, index=False)\n",
        "    rmse = np.sqrt(mean_squared_error(all_preds_df['rating'], all_preds_df['score'])) if not all_preds_df.empty else np.nan\n",
        "    return rmse\n",
        "\n",
        "# === Step 6: Run Evaluation and Save Predictions ===\n",
        "user_rmse = evaluate_model_and_save(test_ratings, user_item_matrix, global_mean, user_bias, item_bias, user_sim_matrix, 'user', \"ubcf_predictions.csv\")\n",
        "item_rmse = evaluate_model_and_save(test_ratings, user_item_matrix, global_mean, user_bias, item_bias, item_sim_matrix, 'item', \"ibcf_predictions.csv\")\n",
        "dummy_rmse = np.sqrt(mean_squared_error(test_ratings['rating'], [global_mean] * len(test_ratings)))\n",
        "\n",
        "print(f\"User-Based CF RMSE: {user_rmse:.4f}\")\n",
        "print(f\"Item-Based CF RMSE: {item_rmse:.4f}\")\n",
        "print(f\"Dummy Predictor RMSE: {dummy_rmse:.4f}\")\n",
        "\n",
        "# === Step 7: Top-N for One User ===\n",
        "user_id = 5549\n",
        "user_recs = recommend_memory_based(user_id, user_item_matrix, global_mean, user_bias, item_bias, user_sim_matrix, 'user', top_n=50)\n",
        "item_recs = recommend_memory_based(user_id, user_item_matrix, global_mean, user_bias, item_bias, item_sim_matrix, 'item', top_n=50)\n",
        "\n",
        "user_recs = user_recs.merge(movies, on='movieId', how='left')\n",
        "item_recs = item_recs.merge(movies, on='movieId', how='left')\n",
        "\n",
        "print(\"\\nTop 10 User-Based CF Recommendations:\")\n",
        "print(user_recs[['movieId', 'title', 'score']].head(10))\n",
        "\n",
        "print(\"\\nTop 10 Item-Based CF Recommendations:\")\n",
        "print(item_recs[['movieId', 'title', 'score']].head(10))\n",
        "\n",
        "\n",
        "\n",
        "# # === Load Ratings and Movies Data ===\n",
        "# ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "#                       names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "# movies = pd.read_csv(\"movies_enriched_full.csv\")[['movieId', 'title', 'year']]\n",
        "\n",
        "# Convert timestamp to datetime\n",
        "ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')\n",
        "\n",
        "# Estimate release date from year (assume July 1st if missing)\n",
        "movies['release_date'] = pd.to_datetime(movies['year'].fillna(2000).astype(int).astype(str) + \"-07-01\", errors='coerce')\n",
        "\n",
        "# Merge for temporal info\n",
        "ratings = ratings.merge(movies[['movieId', 'release_date']], on='movieId', how='left')\n",
        "\n",
        "# === User-Level Features ===\n",
        "now = ratings['timestamp'].max()\n",
        "user_stats = ratings.groupby(\"userId\").agg(\n",
        "    user_avg_rating=('rating', 'mean'),\n",
        "    user_rating_std=('rating', 'std'),\n",
        "    user_rating_count=('rating', 'count'),\n",
        "    user_last_ts=('timestamp', 'max')\n",
        ").reset_index()\n",
        "user_stats['user_days_since_active'] = (now - user_stats['user_last_ts']).dt.days\n",
        "user_stats.drop(columns='user_last_ts', inplace=True)\n",
        "\n",
        "# === Movie-Level Features ===\n",
        "movie_stats = ratings.groupby(\"movieId\").agg(\n",
        "    movie_avg_rating=('rating', 'mean'),\n",
        "    movie_rating_std=('rating', 'std'),\n",
        "    movie_rating_count=('rating', 'count')\n",
        ").reset_index()\n",
        "\n",
        "# === Temporal Features ===\n",
        "ratings['days_since_release'] = (ratings['timestamp'] - ratings['release_date']).dt.days\n",
        "ratings['hour'] = ratings['timestamp'].dt.hour\n",
        "ratings['dayofweek'] = ratings['timestamp'].dt.dayofweek\n",
        "ratings['month'] = ratings['timestamp'].dt.month\n",
        "\n",
        "# === Aggregate Temporal Features per Movie ===\n",
        "temporal_features = ratings.groupby(\"movieId\").agg(\n",
        "    avg_days_since_release=('days_since_release', 'mean'),\n",
        "    mode_hour=('hour', lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan),\n",
        "    mode_dayofweek=('dayofweek', lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan),\n",
        "    mode_month=('month', lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
        ").reset_index()\n",
        "\n",
        "# === Combine Movie Features ===\n",
        "movie_features = movie_stats.merge(temporal_features, on='movieId', how='left')\n",
        "\n",
        "# === Save Outputs ===\n",
        "user_stats.to_csv(\"meta_user_features.csv\", index=False)\n",
        "movie_features.to_csv(\"meta_movie_features.csv\", index=False)\n",
        "\n",
        "print(\"Saved meta-user features to: meta_user_features.csv\")\n",
        "print(\"Saved meta-movie features to: meta_movie_features.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6d3SP0OolH0"
      },
      "source": [
        "## **Model-Based Filtering:**\n",
        "\n",
        "  * *SVD (Surprise)*: Learns latent features from the rating matrix.\n",
        "  * *ALS (PySpark)*: Scalable factorization method for large datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dCPxrNMnQkh"
      },
      "source": [
        "### **Module 9: Model-Based Collaborative Filtering (SVD using Surprise)**\n",
        "\n",
        "**Purpose:**\n",
        "Use matrix factorization (SVD) to learn latent user/item features from the rating matrix.\n",
        "\n",
        "**Application:**\n",
        "- Accurate, scalable recommendations for sparse datasets using user/item embeddings.\n",
        "- Suitable for small to medium datasets.\n",
        "- Optimized via `GridSearchCV` for hyperparameter tuning.\n",
        "- Good interpretability of latent factors per user and item.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQVXyqXBMFes",
        "outputId": "31f02f8d-264d-443a-a9d3-6a025a707b18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best RMSE: 0.8816, Params: {'n_factors': 50, 'lr_all': 0.005, 'reg_all': 0.02}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200042/200042 [00:01<00:00, 129043.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 0.8706\n",
            "Saved: svd_surprise_predictions.csv\n",
            "\n",
            "Generating Top-N for User 5549...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3673/3673 [00:00<00:00, 157224.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 Recommendations:\n",
            "   movieId                                        title  pred_rating\n",
            "0      318             Shawshank Redemption, The (1994)     4.318273\n",
            "1     2905                               Sanjuro (1962)     4.293852\n",
            "2      953                 It's a Wonderful Life (1946)     4.269907\n",
            "3      527                      Schindler's List (1993)     4.258500\n",
            "4     3469                      Inherit the Wind (1960)     4.238201\n",
            "5     2324   Life Is Beautiful (La Vita è bella) (1997)     4.206947\n",
            "6     1950              In the Heat of the Night (1967)     4.184346\n",
            "7     3134  Grand Illusion (Grande illusion, La) (1937)     4.169681\n",
            "8      928                               Rebecca (1940)     4.162809\n",
            "9     3030                               Yojimbo (1961)     4.151024\n",
            "Saved: top50_svd_surprise_user_5549.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: svd_surprise_predictions_with_embeddings.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from surprise import Dataset, Reader, SVD\n",
        "from surprise.model_selection import train_test_split, GridSearchCV\n",
        "from surprise.accuracy import rmse as surprise_rmse\n",
        "from tqdm import tqdm\n",
        "\n",
        "# === Step 1: Load ratings\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "\n",
        "# === Step 2: Prepare Surprise data\n",
        "reader = Reader(rating_scale=(0.5, 5.0))\n",
        "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "# === Step 3: Tune SVD Model\n",
        "param_grid = {\n",
        "    'n_factors': [50, 100],\n",
        "    'lr_all': [0.005, 0.01],\n",
        "    'reg_all': [0.02, 0.1]\n",
        "}\n",
        "gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
        "gs.fit(data)\n",
        "best_svd_model = gs.best_estimator['rmse']\n",
        "print(f\"Best RMSE: {gs.best_score['rmse']:.4f}, Params: {gs.best_params['rmse']}\")\n",
        "\n",
        "# === Step 4: Train/Test Split and Evaluate\n",
        "trainset, testset = train_test_split(data, test_size=0.2)\n",
        "best_svd_model.fit(trainset)\n",
        "\n",
        "predictions = [best_svd_model.predict(uid, iid, r_ui=rui) for uid, iid, rui in tqdm(testset)]\n",
        "svd_rmse = surprise_rmse(predictions)\n",
        "\n",
        "# === Save full predictions with tag\n",
        "pred_df = pd.DataFrame(predictions, columns=['uid', 'iid', 'rui', 'est', 'details'])\n",
        "pred_df = pred_df.rename(columns={'uid': 'userId', 'iid': 'movieId', 'rui': 'true_rating', 'est': 'pred_rating'})\n",
        "pred_df['model'] = 'svd_surprise'\n",
        "pred_df.to_csv(\"svd_surprise_predictions.csv\", index=False)\n",
        "print(\"Saved: svd_surprise_predictions.csv\")\n",
        "\n",
        "# === Step 5: Top-N for User 5549\n",
        "target_user = 5549\n",
        "all_movie_ids = ratings['movieId'].unique()\n",
        "rated_movie_ids = ratings[ratings['userId'] == target_user]['movieId'].unique()\n",
        "unrated_movie_ids = [mid for mid in all_movie_ids if mid not in rated_movie_ids]\n",
        "\n",
        "print(f\"\\nGenerating Top-N for User {target_user}...\")\n",
        "top_preds = [(mid, best_svd_model.predict(target_user, mid).est) for mid in tqdm(unrated_movie_ids)]\n",
        "top_50_df = pd.DataFrame(top_preds, columns=['movieId', 'pred_rating'])\n",
        "top_50_df = top_50_df.sort_values(by='pred_rating', ascending=False).head(50)\n",
        "top_50_df['userId'] = target_user\n",
        "top_50_df['model'] = 'svd_surprise'\n",
        "top_50_df = top_50_df[['userId', 'movieId', 'pred_rating', 'model']]\n",
        "\n",
        "# Optional: Merge with movie titles\n",
        "movies = pd.read_csv(\"movies_enriched_full.csv\")[['movieId', 'title']]\n",
        "top_50_df = top_50_df.merge(movies, on='movieId', how='left')\n",
        "\n",
        "print(\"\\nTop 10 Recommendations:\")\n",
        "print(top_50_df[['movieId', 'title', 'pred_rating']].head(10))\n",
        "\n",
        "# === Save Top-N recommendations\n",
        "top_50_df.to_csv(\"top50_svd_surprise_user_5549.csv\", index=False)\n",
        "print(\"Saved: top50_svd_surprise_user_5549.csv\")\n",
        "\n",
        "# === Step 6: Extract Latent Embeddings for Meta-Learner\n",
        "# User factors\n",
        "user_factors = {\n",
        "    best_svd_model.trainset.to_raw_uid(i): np.array(f)\n",
        "    for i, f in enumerate(best_svd_model.pu)\n",
        "}\n",
        "user_emb_df = pd.DataFrame.from_dict(user_factors, orient=\"index\")\n",
        "user_emb_df.index.name = \"userId\"\n",
        "user_emb_df.reset_index(inplace=True)\n",
        "user_emb_df.columns = [\"userId\"] + [f\"user_emb_{i}\" for i in range(best_svd_model.n_factors)]\n",
        "\n",
        "# Item factors\n",
        "item_factors = {\n",
        "    best_svd_model.trainset.to_raw_iid(i): np.array(f)\n",
        "    for i, f in enumerate(best_svd_model.qi)\n",
        "}\n",
        "item_emb_df = pd.DataFrame.from_dict(item_factors, orient=\"index\")\n",
        "item_emb_df.index.name = \"movieId\"\n",
        "item_emb_df.reset_index(inplace=True)\n",
        "item_emb_df.columns = [\"movieId\"] + [f\"movie_emb_{i}\" for i in range(best_svd_model.n_factors)]\n",
        "\n",
        "# Merge embeddings into predictions\n",
        "pred_df = pred_df.merge(user_emb_df, on=\"userId\", how=\"left\")\n",
        "pred_df = pred_df.merge(item_emb_df, on=\"movieId\", how=\"left\")\n",
        "\n",
        "# Save for meta-learner\n",
        "pred_df.to_csv(\"svd_surprise_predictions_with_embeddings.csv\", index=False)\n",
        "print(\"Saved: svd_surprise_predictions_with_embeddings.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeCmPC_DnZip"
      },
      "source": [
        "### **Model-Based Collaborative Filtering (ALS using PySpark)**\n",
        "\n",
        "**Purpose:**\n",
        "Use Alternating Least Squares (ALS) to learn latent user/item features at scale.\n",
        "\n",
        "**Application:**\n",
        "- Distributed recommendation system for large-scale datasets.\n",
        "- Runs on Apache Spark for horizontal scalability.\n",
        "- Handles sparsity well using factorization.\n",
        "- Suited for real-time, production-level systems with massive data.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# ALS Model + Embedding Export for Meta-Learner\n",
        "# ============================\n",
        "\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.functions import col\n",
        "import os\n",
        "\n",
        "# --- Start Spark Session ---\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ALSModel\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# --- Load Ratings ---\n",
        "ratings = pd.read_csv(\"ratings.dat\", sep=\"::\", engine=\"python\",\n",
        "                      names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"])\n",
        "ratings_df = spark.createDataFrame(ratings[['userId', 'movieId', 'rating']])\n",
        "\n",
        "# --- Train/Test Split ---\n",
        "(training_df, test_df) = ratings_df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# --- Train ALS Model ---\n",
        "als = ALS(\n",
        "    userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\",\n",
        "    rank=20, maxIter=10, regParam=0.1,\n",
        "    coldStartStrategy=\"drop\", nonnegative=True\n",
        ")\n",
        "als_model = als.fit(training_df)\n",
        "\n",
        "# --- Predict on Test Set ---\n",
        "test_predictions = als_model.transform(test_df)\n",
        "\n",
        "# --- Evaluate RMSE ---\n",
        "evaluator = RegressionEvaluator(\n",
        "    metricName='rmse',\n",
        "    labelCol='rating',\n",
        "    predictionCol='prediction'\n",
        ")\n",
        "rmse_score = evaluator.evaluate(test_predictions)\n",
        "\n",
        "# --- Save Prediction Data ---\n",
        "pred_pd = test_predictions.select('userId', 'movieId', 'rating', 'prediction').toPandas()\n",
        "pred_pd = pred_pd.rename(columns={'rating': 'true_rating', 'prediction': 'pred_rating'})\n",
        "pred_pd['model'] = 'ALS (PySpark)'\n",
        "pred_pd.to_csv(\"als_predictions_test.csv\", index=False)\n",
        "\n",
        "print(f\"\\nFinal RMSE on Test Set: {rmse_score:.4f}\")\n",
        "print(pred_pd[['userId', 'movieId', 'true_rating', 'pred_rating', 'model']].head())\n",
        "\n",
        "# ============================\n",
        "# Extract ALS Embeddings\n",
        "# ============================\n",
        "\n",
        "# --- User Factors ---\n",
        "user_factors_df = als_model.userFactors.toPandas()\n",
        "user_factors_df = user_factors_df.rename(columns={\"id\": \"userId\"})\n",
        "user_latent_df = pd.DataFrame(user_factors_df[\"features\"].tolist())\n",
        "user_latent_df[\"userId\"] = user_factors_df[\"userId\"]\n",
        "\n",
        "# --- Item Factors ---\n",
        "item_factors_df = als_model.itemFactors.toPandas()\n",
        "item_factors_df = item_factors_df.rename(columns={\"id\": \"movieId\"})\n",
        "item_latent_df = pd.DataFrame(item_factors_df[\"features\"].tolist())\n",
        "item_latent_df[\"movieId\"] = item_factors_df[\"movieId\"]\n",
        "\n",
        "# --- Merge Embeddings into Prediction Data ---\n",
        "merged_df = pred_pd.merge(user_latent_df, on=\"userId\", how=\"left\")\n",
        "merged_df = merged_df.merge(item_latent_df, on=\"movieId\", how=\"left\", suffixes=(\"_user\", \"_movie\"))\n",
        "\n",
        "# --- Save Meta-Learner Dataset ---\n",
        "merged_df.to_csv(\"als_meta_features.csv\", index=False)\n",
        "print(\"\\nSaved ALS-based meta-feature dataset: als_meta_features.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUdW9d5QYxy6",
        "outputId": "069ab7ec-5cbb-408d-ce26-57e25975c32d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final RMSE on Test Set: 0.8725\n",
            "   userId  movieId  true_rating  pred_rating          model\n",
            "0     148       11            5     4.055593  ALS (PySpark)\n",
            "1     148       17            4     3.777626  ALS (PySpark)\n",
            "2     148      107            4     3.382518  ALS (PySpark)\n",
            "3     148      165            3     3.875768  ALS (PySpark)\n",
            "4     148      185            3     3.476842  ALS (PySpark)\n",
            "\n",
            "Saved ALS-based meta-feature dataset: als_meta_features.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOPkUwmWieTt"
      },
      "source": [
        "## Meta-Learner Preprocessing\n",
        "\n",
        "1. **Sample Subset of Predictions**\n",
        "   Instead of merging all 2.2 million+ rows, we’ll sample a manageable subset per model (e.g., 10K rows per model) for meta-learner training.\n",
        "\n",
        "2. **Join Movie Metadata**\n",
        "   Merge only this sampled data with the selected metadata columns.\n",
        "\n",
        "3. **Feature Engineering**\n",
        "\n",
        "   * **Target**: `true_rating`\n",
        "   * **Features**:\n",
        "\n",
        "     * `pred_rating` from each model (wide format pivoted per model)\n",
        "     * Movie metadata:\n",
        "\n",
        "       * `vote_average` (numeric → scale)\n",
        "       * `vote_count` (numeric → scale)\n",
        "       * `genres`, `top_3_cast`, `directors`, `keywords` → use **TF-IDF vectorization** (not dummy encoding).\n",
        "\n",
        "4. **Model**: Use **XGBoost** as the meta-learner (handles missing values, scales well, supports tree-based split on sparse data)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByEFskVij2re"
      },
      "source": [
        "#### Step 1: Load and Merge Each Prediction File with Metadata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load movie metadata\n",
        "movie_df = pd.read_csv(\"movies_enriched_full.csv\")\n",
        "\n",
        "# Define the 12 CSV files and their merged output names\n",
        "prediction_files = {\n",
        "    \"als_meta_features\": \"als_meta_features.csv\",\n",
        "    \"cbf_meta_features_binary\": \"cbf_meta_features_binary.csv\",\n",
        "    \"cbf_meta_features_count\": \"cbf_meta_features_count.csv\",\n",
        "    \"cbf_meta_features_tfidf\": \"cbf_meta_features_tfidf.csv\",\n",
        "    \"ibcf_predictions\": \"ibcf_predictions.csv\",\n",
        "    \"meta_movie_features\": \"meta_movie_features.csv\",\n",
        "    \"meta_user_features\": \"meta_user_features.csv\",\n",
        "    \"svd_surprise_embedding\": \"svd_surprise_predictions_with_embeddings.csv\",\n",
        "    \"ubcf_predictions\": \"ubcf_predictions.csv\",\n",
        "    \"cbf_predictions_tfidf\": \"cbf_predictions_tfidf.csv\",\n",
        "    \"cbf_predictions_binary\": \"cbf_predictions_binary.csv\",\n",
        "    \"cbf_predictions_count\": \"cbf_predictions_count.csv\",\n",
        "}\n",
        "\n",
        "# Merge and save each file\n",
        "for model_name, file_path in prediction_files.items():\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        if 'movieId' in df.columns:\n",
        "            merged = pd.merge(df, movie_df, on=\"movieId\", how=\"left\")\n",
        "        else:\n",
        "            merged = df  # Skip merging if movieId is missing\n",
        "\n",
        "        output_path = f\"{model_name}_merged.csv\"\n",
        "        merged.to_csv(output_path, index=False)\n",
        "        print(f\"Saved: {output_path} with shape {merged.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5cn1VKiocqj",
        "outputId": "f7e0a51a-abaa-42ca-f7dc-414bd0ffa51d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved: als_meta_features_merged.csv with shape (199718, 60)\n",
            "✅ Saved: cbf_meta_features_binary_merged.csv with shape (202451, 21)\n",
            "✅ Saved: cbf_meta_features_count_merged.csv with shape (202451, 21)\n",
            "✅ Saved: cbf_meta_features_tfidf_merged.csv with shape (202451, 21)\n",
            "✅ Saved: ibcf_predictions_merged.csv with shape (200016, 20)\n",
            "✅ Saved: meta_movie_features_merged.csv with shape (3706, 23)\n",
            "✅ Saved: meta_user_features_merged.csv with shape (6040, 5)\n",
            "✅ Saved: svd_surprise_embedding_merged.csv with shape (200042, 121)\n",
            "✅ Saved: ubcf_predictions_merged.csv with shape (200016, 20)\n",
            "✅ Saved: cbf_predictions_tfidf_merged.csv with shape (202451, 20)\n",
            "✅ Saved: cbf_predictions_binary_merged.csv with shape (202451, 20)\n",
            "✅ Saved: cbf_predictions_count_merged.csv with shape (202451, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I919p6k2j6SR"
      },
      "source": [
        "#### Hybrid XGBoost Meta-Learner Model for Movie Rating Prediction\n",
        "\n",
        "**1. Objective**\n",
        "\n",
        "The objective of this experiment was to develop a **meta-learning model** that integrates the predictions from multiple base recommenders—including collaborative filtering, content-based filtering, matrix factorization, and embedding-based models—into a single, higher-performing regressor. This meta-model aims to **leverage the strengths of all sub-models** and contextual metadata (e.g., cast, genres, keywords, and embeddings) to produce more accurate predictions of user ratings for movies.\n",
        "\n",
        "**2. Methodology**\n",
        "\n",
        "To construct the meta-learner, we followed a comprehensive pipeline, which included:\n",
        "\n",
        "**a. Data Aggregation**\n",
        "\n",
        "We began by loading the predictions and meta-features from twelve pre-trained models and feature sources:\n",
        "\n",
        "* `als_meta_features.csv`\n",
        "* `cbf_meta_features_binary.csv`\n",
        "* `cbf_meta_features_count.csv`\n",
        "* `cbf_meta_features_tfidf.csv`\n",
        "* `ibcf_predictions.csv`\n",
        "* `meta_movie_features.csv`\n",
        "* `meta_user_features.csv`\n",
        "* `svd_surprise_predictions_with_embeddings.csv`\n",
        "* `ubcf_predictions.csv`\n",
        "* Three direct CBF model outputs for comparison\n",
        "\n",
        "All prediction files were merged and aligned on `movieId` and `userId`.\n",
        "\n",
        "**b. Feature Engineering**\n",
        "\n",
        "We enriched the merged dataset with metadata from `movies_enriched_full.csv` and combined columns like `tmdb_genres`, `top_3_cast`, `directors`, and `keywords` into a unified textual feature called `cbf_features`.\n",
        "\n",
        "A **TF-IDF vectorizer** (limited to 3,000 features to reduce RAM usage) was used to vectorize these combined text fields. In parallel, all relevant numeric features (excluding IDs, text columns, and any field containing \"user\\_\") were scaled using `StandardScaler`.\n",
        "\n",
        "These two feature sets—sparse TF-IDF and scaled numeric features—were **horizontally stacked** into a combined matrix for model training.\n",
        "\n",
        "**c. Model Tuning**\n",
        "\n",
        "An **XGBoost Regressor** was chosen as the meta-model due to its robustness, support for mixed data types, and GPU acceleration. The model was tuned using `GridSearchCV` with 5-fold cross-validation. The grid included variations of:\n",
        "\n",
        "* `max_depth`: 4\n",
        "* `learning_rate`: 0.05, 0.1\n",
        "* `n_estimators`: 100, 200\n",
        "* `reg_alpha`: 0\n",
        "* `reg_lambda`: 1, 2\n",
        "* `subsample`: 0.8\n",
        "* `colsample_bytree`: 0.8\n",
        "\n",
        "The **grid search** was optimized for **negative RMSE**, and the best parameters were selected after evaluating 55 combinations across all folds.\n",
        "\n",
        "**d. Final Model Training**\n",
        "\n",
        "Using the best parameters from tuning, we trained the final XGBoost model with:\n",
        "\n",
        "* `max_depth = 10`\n",
        "* `learning_rate = 0.1`\n",
        "* `n_estimators = 200`\n",
        "* `reg_alpha = 0`\n",
        "* `reg_lambda = 1`\n",
        "* `subsample = 0.8`\n",
        "* `colsample_bytree = 0.8`\n",
        "\n",
        "GPU acceleration (`device='cuda'`) was used throughout to reduce training time on Google Colab’s T4 GPU environment.\n",
        "\n",
        "**3. Evaluation**\n",
        "\n",
        "The final model was evaluated on a held-out test set (20%) using the following metrics:\n",
        "\n",
        "* **Root Mean Squared Error (RMSE)**: *Indicates average prediction error magnitude.*\n",
        "* **R² Score**: *Shows how much variance in the true ratings the model was able to explain.*\n",
        "\n",
        "Predictions were saved to `meta_model_bestparams_predictions.csv` for further analysis or re-attachment to user/movie metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 0: Setup ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.sparse import hstack\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Step 1: Load and merge prediction files ---\n",
        "prediction_files = {\n",
        "    \"als_meta_features\": \"als_meta_features.csv\",\n",
        "    \"cbf_meta_features_binary\": \"cbf_meta_features_binary.csv\",\n",
        "    \"cbf_meta_features_count\": \"cbf_meta_features_count.csv\",\n",
        "    \"cbf_meta_features_tfidf\": \"cbf_meta_features_tfidf.csv\",\n",
        "    \"ibcf_predictions\": \"ibcf_predictions.csv\",\n",
        "    \"meta_movie_features\": \"meta_movie_features.csv\",\n",
        "    \"meta_user_features\": \"meta_user_features.csv\",\n",
        "    \"svd_surprise_embedding\": \"svd_surprise_predictions_with_embeddings.csv\",\n",
        "    \"ubcf_predictions\": \"ubcf_predictions.csv\",\n",
        "    \"cbf_predictions_tfidf\": \"cbf_predictions_tfidf.csv\",\n",
        "    \"cbf_predictions_binary\": \"cbf_predictions_binary.csv\",\n",
        "    \"cbf_predictions_count\": \"cbf_predictions_count.csv\",\n",
        "}\n",
        "\n",
        "merged_dfs = []\n",
        "for model_name, file_path in prediction_files.items():\n",
        "    df = pd.read_csv(file_path)\n",
        "    df[\"model\"] = model_name\n",
        "    merged_dfs.append(df)\n",
        "\n",
        "df = pd.concat(merged_dfs, ignore_index=True)\n",
        "df = df.dropna(subset=[\"true_rating\"]).reset_index(drop=True)\n",
        "\n",
        "# Save userId and movieId for reattachment\n",
        "id_df = df[[\"userId\", \"movieId\"]].reset_index(drop=True)\n",
        "\n",
        "# --- Step 1.5: Merge movie metadata to get cbf columns ---\n",
        "movies_df = pd.read_csv(\"movies_enriched_full.csv\")\n",
        "df = pd.merge(df, movies_df, on=\"movieId\", how=\"left\")\n",
        "\n",
        "# --- Step 2: Combine metadata into cbf_features ---\n",
        "text_cols = [\"tmdb_genres\", \"top_3_cast\", \"directors\", \"keywords\"]\n",
        "for col in text_cols:\n",
        "    df[col] = df[col].fillna(\"\").astype(str)\n",
        "df[\"cbf_features\"] = df[text_cols].agg(\" \".join, axis=1)\n",
        "\n",
        "# --- Step 3: TF-IDF Vectorization ---\n",
        "tfidf = TfidfVectorizer(max_features=3000, stop_words=\"english\")\n",
        "X_text = tfidf.fit_transform(df[\"cbf_features\"])  # sparse matrix\n",
        "\n",
        "# --- Step 4: Numeric features + scaling ---\n",
        "exclude_cols = {\n",
        "    \"userId\", \"movieId\", \"true_rating\", \"cbf_features\", \"details\",\n",
        "    \"title\", \"model\", \"tmdb_genres\", \"top_3_cast\", \"directors\", \"keywords\"\n",
        "}\n",
        "\n",
        "# Add columns containing \"_user\" or \"user_\"\n",
        "# exclude_cols.update({col for col in df.columns if \"_user\" in col or \"user_\" in col})\n",
        "\n",
        "\n",
        "all_numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "numeric_cols = [col for col in all_numeric_cols if col not in exclude_cols]\n",
        "\n",
        "X_numeric = df[numeric_cols].fillna(0).astype(np.float32).values\n",
        "scaler = StandardScaler()\n",
        "X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
        "\n",
        "# --- Step 5: Combine features ---\n",
        "X_combined = hstack([X_text, X_numeric_scaled]).astype(np.float32)\n",
        "y = df[\"true_rating\"].values.astype(np.float32)\n",
        "\n",
        "# --- Step 6: Train/Test Split (with IDs) ---\n",
        "X_train, X_test, y_train, y_test, id_train, id_test = train_test_split(\n",
        "    X_combined, y, id_df, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --- Step 7: Train Final Model with Best Params ---\n",
        "xgb_model = XGBRegressor(\n",
        "    objective=\"reg:squarederror\",\n",
        "    tree_method=\"hist\",\n",
        "    device=\"cuda\",\n",
        "    max_depth=10,\n",
        "    learning_rate=0.1,\n",
        "    n_estimators=200,\n",
        "    reg_alpha=0,\n",
        "    reg_lambda=1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    eval_metric=\"rmse\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# params = {\n",
        "#     \"objective\": \"reg:squarederror\",\n",
        "#     \"tree_method\": \"hist\",             # use hist\n",
        "#     \"device\": \"cuda\",                  # enables GPU\n",
        "#     \"max_depth\": 6,\n",
        "#     \"eta\": 0.1,\n",
        "#     \"eval_metric\": \"rmse\"\n",
        "# }\n",
        "\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# --- Step 8: Predict and Save ---\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "pred_df = id_test.copy()\n",
        "pred_df[\"pred_rating\"] = y_pred\n",
        "pred_df[\"true_rating\"] = y_test\n",
        "pred_df.to_csv(\"meta_model_bestparams_predictions.csv\", index=False)\n",
        "\n",
        "# --- Step 9: Evaluate ---\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"\\nFinal Model (Best Params)\")\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n",
        "print(pred_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rJ85qO3ac3I",
        "outputId": "d04cb594-5ee4-4688-ef8e-993ddb3a8265"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Model (Best Params)\n",
            "RMSE: 0.9120\n",
            "R² Score: 0.3351\n",
            "         userId  movieId  pred_rating  true_rating\n",
            "1906164  2865.0   2485.0     3.384829          3.0\n",
            "1118208  3867.0   1073.0     4.080280          4.0\n",
            "1296837  3610.0   1739.0     2.383415          1.0\n",
            "1750018  4155.0   1371.0     2.927920          4.0\n",
            "888026   2119.0   1619.0     3.458188          4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple"
      ],
      "metadata": {
        "id": "gUw30zyqoRAr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMDYya4Qie_u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load movie metadata\n",
        "movie_df = pd.read_csv(\"movies_enriched_full.csv\")\n",
        "\n",
        "# List of prediction files with desired output name\n",
        "prediction_files = {\n",
        "    \"ubcf\": \"ubcf_predictions_simple.csv\",\n",
        "    \"svd\": \"svd_surprise_predictions_simple.csv\",\n",
        "    \"ibcf\": \"ibcf_predictions_simple.csv\",\n",
        "    \"cbf_tfidf\": \"cbf_predictions_tfidf_simple.csv\",\n",
        "    \"cbf_count\": \"cbf_predictions_count_simple.csv\",\n",
        "    \"cbf_binary\": \"cbf_predictions_binary_simple.csv\",\n",
        "    \"als\": \"als_pyspark_predictions_simple.csv\"\n",
        "}\n",
        "\n",
        "# Merge and save each separately\n",
        "for model_name, file_path in prediction_files.items():\n",
        "\n",
        "    # print(f\"\\{df} with shape {df.info()}\")\n",
        "    merged = pd.merge(df, movie_df, on=\"movieId\", how=\"left\")\n",
        "    output_path = f\"/content/{model_name}_merged.csv\"\n",
        "    merged.to_csv(output_path, index=False)\n",
        "    print(f\"Saved: {output_path} with shape {merged.shape}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 0: Setup ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.sparse import hstack\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# --- Step 1: Load and merge prediction files ---\n",
        "merged_dfs = []\n",
        "for model_name in prediction_files.keys():\n",
        "    df = pd.read_csv(f\"{model_name}_merged.csv\")\n",
        "    df[\"model\"] = model_name\n",
        "    merged_dfs.append(df)\n",
        "\n",
        "df = pd.concat(merged_dfs, ignore_index=True)\n",
        "\n",
        "# --- Step 2: Clean dataset ---\n",
        "df = df.dropna(subset=[\"true_rating\"]).reset_index(drop=True)\n",
        "\n",
        "# Save userId and movieId for reattachment later\n",
        "id_df = df[[\"userId\", \"movieId\"]].reset_index(drop=True)\n",
        "\n",
        "# --- Step 3: Combine metadata into CBF text features ---\n",
        "text_cols = [\"tmdb_genres\", \"top_3_cast\", \"directors\", \"keywords\"]\n",
        "for col in text_cols:\n",
        "    df[col] = df[col].fillna(\"\").astype(str)\n",
        "\n",
        "df[\"cbf_features\"] = df[text_cols].agg(\" \".join, axis=1)\n",
        "\n",
        "# --- Step 4: TF-IDF Vectorization ---\n",
        "tfidf = TfidfVectorizer(max_features=5000)\n",
        "X_text = tfidf.fit_transform(df[\"cbf_features\"])  # sparse matrix\n",
        "\n",
        "# --- Step 5: Numeric features and scaling ---\n",
        "numeric_cols = [\"vote_average\", \"vote_count\", \"pred_rating\"]\n",
        "X_numeric = df[numeric_cols].fillna(0).values\n",
        "scaler = StandardScaler()\n",
        "X_numeric_scaled = scaler.fit_transform(X_numeric)\n",
        "\n",
        "# --- Step 6: Combine TF-IDF and numeric features ---\n",
        "X_combined = hstack([X_text, X_numeric_scaled])  # final input matrix (sparse)\n",
        "y = df[\"true_rating\"].values  # target variable\n",
        "\n",
        "# --- Step 7: Train/test split (also split ID columns) ---\n",
        "X_train, X_test, y_train, y_test, id_train, id_test = train_test_split(\n",
        "    X_combined, y, id_df, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --- Step 8: Train XGBoost model (GPU) ---\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "params = {\n",
        "    \"objective\": \"reg:squarederror\",\n",
        "    \"tree_method\": \"hist\",             # use hist\n",
        "    \"device\": \"cuda\",                  # enables GPU\n",
        "    \"max_depth\": 6,\n",
        "    \"eta\": 0.1,\n",
        "    \"eval_metric\": \"rmse\"\n",
        "}\n",
        "\n",
        "\n",
        "model = xgb.train(params, dtrain, num_boost_round=100)\n",
        "\n",
        "# --- Step 9: Predict and reattach IDs ---\n",
        "y_pred = model.predict(dtest)\n",
        "\n",
        "pred_df = id_test.copy()\n",
        "pred_df[\"pred_rating\"] = y_pred\n",
        "pred_df[\"true_rating\"] = y_test\n",
        "\n",
        "# --- Step 10: Evaluate ---\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n",
        "print(pred_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKwvbNoAK2gj",
        "outputId": "756a1911-41d1-4c32-f162-9c270cd01a14"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ RMSE: 0.8846\n",
            "✅ R² Score: 0.3747\n",
            "         userId  movieId  pred_rating  true_rating\n",
            "1318706    3032     3421     4.459865            5\n",
            "662395     1836      367     4.028944            3\n",
            "336973      261      505     2.592825            2\n",
            "564891     3216     1527     3.222407            3\n",
            "1797252    4478      367     2.913699            2\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}